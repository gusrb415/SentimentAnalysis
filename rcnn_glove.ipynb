{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    References:  https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html\n",
    "                 https://github.com/keras-team/keras/blob/master/examples\n",
    "                 http://www.aaai.org/ocs/index.php/AAAI/AAAI15/paper/view/9745\n",
    "                 https://github.com/airalcorn2/Recurrent-Convolutional-Neural-Network-Text-Classifier/blob/master/recurrent_convolutional_keras.py\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.initializers import Constant\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from keras import backend\n",
    "from keras.layers import Conv1D, Dense, Input, Lambda, CuDNNLSTM, CuDNNGRU\n",
    "from keras.layers import Bidirectional, GlobalMaxPooling1D, MaxPooling1D, Dropout, SpatialDropout1D\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english') + list(string.punctuation))\n",
    "\n",
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    :param text: a doc with multiple sentences, type: str\n",
    "    return a word list, type: list\n",
    "    https://textminingonline.com/dive-into-nltk-part-ii-sentence-tokenize-and-word-tokenize\n",
    "    e.g.\n",
    "    Input: 'It is a nice day. I am happy.'\n",
    "    Output: ['it', 'is', 'a', 'nice', 'day', 'i', 'am', 'happy']\n",
    "    \"\"\"\n",
    "    tokens = []\n",
    "    for word in nltk.word_tokenize(text):\n",
    "        word = word.lower()\n",
    "        if word not in stop_words and not word.isnumeric():\n",
    "            tokens.append(word)\n",
    "    return tokens\n",
    "\n",
    "def clean_str(strings):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for dataset\n",
    "    Every dataset is lower cased except\n",
    "    \"\"\"\n",
    "    new_strings = []\n",
    "    for string in strings:\n",
    "        string = re.sub(r\"\\\\\", \"\", string)\n",
    "        string = re.sub(r\"\\'\", \"\", string)\n",
    "        string = re.sub(r\"\\\"\", \"\", string)\n",
    "        string = \" \".join(tokenize(string))\n",
    "        new_strings.append(string)\n",
    "    return new_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "student back neck constant strain dr serrick manages set straight every time highly recommend thumper treatment really great muscle pain relief overall would say dr serrick knowledgeable empathetic thorough highly recommended\n",
      "stayed football game university phoenix stadium 10mins drive rooms average fine overnight stay great service good breakfast ... bacon biscuits gravy yum\n",
      "good salads generous portions either get mexican asian cant break away two\n",
      "experience company growing awesome least memories fallen hard years recently ordered pizza different toppings ordered cheese dont gluten free crust dairy allergy friendly one according allergy website use thin crust- yuck ordered spinach ham pepperoni pineapple sounds like would lot top right yeah..no wasnt even enough spinach cover slice pizza pizza looked like barren wasteland crust sauce wont ordering unless im desperate even ill still go somewhere else\n",
      "easiest furniture purchase great price delivery quick support local business awesome\n",
      "great hotel clean dont like pay resort fee pool wi-fi gym services used found way legally steal people would look better raised room rate instead telling someone theyre charged service didnt use\n",
      "café coffee actually important worse version presse café\n",
      "impressed ever wanting try place finally night happy hour .50 cents beer ... fall seat laughter thought server joking wasnt .50 cents ... .. ya kidding didnt even ask wine mixed drinks didnt want choke water beers whole ordered salmon bit cooked dried nothing special pizza app average barros pizza next door much rather barros hope make changes new owner got ta something quick doubt back\n",
      "first night vegas wanted something simple dinner place try beer mac cheese starter good great flavor perfect size start finished gotham pizza good cooked perfectly nice think crispy crust service also good attentive youre wanting slice pizza give place try\n",
      "reviews 1-star variety coincidence shock absolutely worst service ever ive unfortunate experience dealing different receptionists different plumbers also referred home warranty thieves american home shield like many others receptionists rude heck couldnt bothered polite made seem like gigantic hassle even answer phone let alone answer questions make appointment placed outrageously long hold every time ive called talking 8-14 minutes time 1-2 minutes also call service use equally rude turn call forwarding 4:30pm even though theyre open 5pm worst service ever could look past horrible customer service job least done correctly professionally alas first plumber came tank leaked needed new one time emergency didnt question permits new codes straps necessary ended paying 400+ even though tank fixed warranty second time months later wasnt getting enough hot water 50gal tank last 33+ minutes 1.5gpm water rate getting 10-12 minutes second plumber showed looking homeless without ability take hot shower job training year hours investigating convinced problem gas control valve later wasnt actually problem come another hour investigation couldnt diagnose problem threw arms said must tank needs replacing next day third different plumber comes doesnt think tank problem investigates 1.5 hours cant figure urge replace tank finally still doesnt fix problem ive spent know many precious hours almost 1,000 get 10-12 minutes warm water dealing horrendous customer service inept professionals thanks union plumbing hope youre business much longer\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for i in texts:\n",
    "    if count == 10:\n",
    "        break\n",
    "    if not \"a-z\" in i:\n",
    "        print(i)\n",
    "        count = count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing word vectors.\n",
      "Found 400000 word vectors.\n",
      "Processing text dataset\n",
      "Found 110000 texts.\n",
      "Found 86127 unique tokens.\n",
      "Shape of data tensor: (110000, 250)\n",
      "Shape of label tensor: (110000, 6)\n",
      "Preparing embedding matrix.\n"
     ]
    }
   ],
   "source": [
    "BASE_DIR = 'data'\n",
    "GLOVE_DIR = os.path.join(BASE_DIR)\n",
    "TEXT_DATA_DIR = os.path.join(BASE_DIR)\n",
    "MAX_SEQUENCE_LENGTH = 250\n",
    "MAX_NUM_WORDS = 30000\n",
    "EMBEDDING_DIM = 100\n",
    "VALIDATION_SPLIT = 0.0909090909090909090909\n",
    "\n",
    "MAX_TOKENS = 30000\n",
    "hidden_dim_1 = 200\n",
    "hidden_dim_2 = 100\n",
    "NUM_CLASSES = 6\n",
    "\n",
    "# first, build index mapping words in the embeddings set\n",
    "# to their embedding vector\n",
    "print('Indexing word vectors.')\n",
    "\n",
    "embeddings_index = {}\n",
    "with open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'), encoding='UTF-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "# second, prepare text samples and their labels\n",
    "print('Processing text dataset')\n",
    "\n",
    "texts = []  # list of text samples\n",
    "labels = []  # list of labels\n",
    "\n",
    "files = ['train.csv', 'valid.csv']\n",
    "for file_name in files:\n",
    "    file = pd.read_csv(os.path.join(TEXT_DATA_DIR, file_name))\n",
    "    for line in file['text']:\n",
    "        texts.append(line)\n",
    "    for label in file['stars']:\n",
    "        labels.append(label)\n",
    "\n",
    "print('Found %s texts.' % len(texts))\n",
    "\n",
    "# finally, vectorize the text samples into a 2D integer tensor\n",
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "texts = clean_str(texts)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "left = list()\n",
    "right = list()\n",
    "for token_list in sequences:\n",
    "    # We shift the document to the right to obtain the left-side contexts.\n",
    "    left.append([MAX_TOKENS] + token_list[:-1])\n",
    "    # We shift the document to the left to obtain the right-side contexts.\n",
    "    right.append(token_list[1:] + [MAX_TOKENS])\n",
    "\n",
    "left_context_as_array = pad_sequences(left, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "right_context_as_array = pad_sequences(right, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "labels = to_categorical(np.asarray(labels))\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "# split the data into a training set and a validation set\n",
    "indices = np.arange(data.shape[0])\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "num_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "\n",
    "x_train_left = left_context_as_array[:-num_validation_samples]\n",
    "x_train_right = right_context_as_array[:-num_validation_samples]\n",
    "x_train = data[:-num_validation_samples]\n",
    "y_train = labels[:-num_validation_samples]\n",
    "\n",
    "x_val_left = right_context_as_array[-num_validation_samples:]\n",
    "x_val_right = right_context_as_array[-num_validation_samples:]\n",
    "x_val = data[-num_validation_samples:]\n",
    "y_val = labels[-num_validation_samples:]\n",
    "\n",
    "print('Preparing embedding matrix.')\n",
    "\n",
    "# prepare embedding matrix\n",
    "num_words = min(MAX_NUM_WORDS, len(word_index)) + 1\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i > MAX_NUM_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "# load pre-trained word embeddings into an Embedding layer\n",
    "# note that we set trainable = False so as to keep the embeddings fixed\n",
    "document = Input(shape=(None,), dtype=\"int32\")\n",
    "left_context = Input(shape=(None,), dtype=\"int32\")\n",
    "right_context = Input(shape=(None,), dtype=\"int32\")\n",
    "\n",
    "embedder = Embedding(num_words,\n",
    "                     EMBEDDING_DIM,\n",
    "                     embeddings_initializer=Constant(embedding_matrix),\n",
    "                     input_length=MAX_SEQUENCE_LENGTH,\n",
    "                     trainable=False)\n",
    "doc_embedding = embedder(document)\n",
    "l_embedding = embedder(left_context)\n",
    "r_embedding = embedder(right_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 100000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "100000/100000 [==============================] - 359s 4ms/step - loss: 1.0116 - acc: 0.5813 - val_loss: 0.9346 - val_acc: 0.6108\n",
      "Epoch 2/10\n",
      "100000/100000 [==============================] - 425s 4ms/step - loss: 0.8999 - acc: 0.6216 - val_loss: 0.9476 - val_acc: 0.6000\n",
      "Epoch 3/10\n",
      "100000/100000 [==============================] - 594s 6ms/step - loss: 0.8590 - acc: 0.6372 - val_loss: 0.9006 - val_acc: 0.6287\n",
      "Epoch 4/10\n",
      "100000/100000 [==============================] - 521s 5ms/step - loss: 0.8282 - acc: 0.6511 - val_loss: 0.8545 - val_acc: 0.6395\n",
      "Epoch 5/10\n",
      "100000/100000 [==============================] - 324s 3ms/step - loss: 0.8101 - acc: 0.6588 - val_loss: 0.8782 - val_acc: 0.6328\n",
      "Epoch 6/10\n",
      "100000/100000 [==============================] - 325s 3ms/step - loss: 0.7928 - acc: 0.6634 - val_loss: 0.8680 - val_acc: 0.6305\n",
      "Epoch 7/10\n",
      "100000/100000 [==============================] - 325s 3ms/step - loss: 0.7819 - acc: 0.6699 - val_loss: 0.8421 - val_acc: 0.6432\n",
      "Epoch 8/10\n",
      "100000/100000 [==============================] - 325s 3ms/step - loss: 0.7656 - acc: 0.6752 - val_loss: 0.8145 - val_acc: 0.6544\n",
      "Epoch 9/10\n",
      "100000/100000 [==============================] - 324s 3ms/step - loss: 0.7574 - acc: 0.6783 - val_loss: 0.8145 - val_acc: 0.6508\n",
      "Epoch 10/10\n",
      "100000/100000 [==============================] - 322s 3ms/step - loss: 0.7478 - acc: 0.6837 - val_loss: 0.7906 - val_acc: 0.6641.7477 - acc: 0. - ETA: 2s - \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2337ce90eb8>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drop_out_r = SpatialDropout1D(0.25)(r_embedding)\n",
    "drop_out_l = SpatialDropout1D(0.25)(l_embedding)\n",
    "forward = Bidirectional(CuDNNGRU(hidden_dim_1, return_sequences=True))(drop_out_l)  # See equation (1).\n",
    "backward = Bidirectional(CuDNNGRU(hidden_dim_1, return_sequences=True, go_backwards=True))(drop_out_r)  # See equation (2).\n",
    "# Keras returns the output sequences in reverse order.\n",
    "backward = Lambda(lambda x: backend.reverse(x, axes=1))(backward)\n",
    "together = concatenate([forward, doc_embedding, backward], axis=2)  # See equation (3).\n",
    "\n",
    "drop_out_c = Dropout(0.25)(together)\n",
    "semantic = Conv1D(hidden_dim_2, kernel_size=5, activation=\"tanh\")(drop_out_c)  # See equation (4).\n",
    "\n",
    "# Keras provides its own max-pooling layers, but they cannot handle variable length input\n",
    "# (as far as I can tell). As a result, I define my own max-pooling layer here.\n",
    "# pool_rnn = Lambda(lambda x: backend.max(x, axis=1), output_shape=(hidden_dim_2,))(semantic)  # See equation (5).\n",
    "pool_rnn = GlobalMaxPooling1D()(semantic)\n",
    "\n",
    "output = Dense(NUM_CLASSES, input_dim=hidden_dim_2, activation=\"softmax\")(pool_rnn)  # See equations (6) and (7).\n",
    "\n",
    "model = Model(inputs=[document, left_context, right_context], outputs=output)\n",
    "\n",
    "optimizer = \n",
    "model.compile(optimizer=\"adadelta\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit([x_train, x_train_left, x_train_right], y_train, epochs=10, verbose=1, batch_size=128, initial_epoch=0,\n",
    "          validation_data=([x_val, x_val_left, x_val_right], y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 100000 samples, validate on 10000 samples\n",
      "Epoch 11/20\n",
      "100000/100000 [==============================] - 221s 2ms/step - loss: 0.7166 - acc: 0.6957 - val_loss: 0.8074 - val_acc: 0.6583\n",
      "Epoch 12/20\n",
      "100000/100000 [==============================] - 222s 2ms/step - loss: 0.7038 - acc: 0.7013 - val_loss: 0.7968 - val_acc: 0.6630\n",
      "Epoch 13/20\n",
      "100000/100000 [==============================] - 221s 2ms/step - loss: 0.6965 - acc: 0.7052 - val_loss: 0.7812 - val_acc: 0.6694\n",
      "Epoch 14/20\n",
      "100000/100000 [==============================] - 222s 2ms/step - loss: 0.6870 - acc: 0.7071 - val_loss: 0.7772 - val_acc: 0.6732\n",
      "Epoch 15/20\n",
      "100000/100000 [==============================] - 223s 2ms/step - loss: 0.6807 - acc: 0.7104 - val_loss: 0.7890 - val_acc: 0.6704\n",
      "Epoch 16/20\n",
      "100000/100000 [==============================] - 223s 2ms/step - loss: 0.6724 - acc: 0.7147 - val_loss: 0.7980 - val_acc: 0.6646\n",
      "Epoch 17/20\n",
      "100000/100000 [==============================] - 223s 2ms/step - loss: 0.6673 - acc: 0.7158 - val_loss: 0.7824 - val_acc: 0.6692\n",
      "Epoch 18/20\n",
      "100000/100000 [==============================] - 223s 2ms/step - loss: 0.6588 - acc: 0.7207 - val_loss: 0.7949 - val_acc: 0.6611\n",
      "Epoch 19/20\n",
      "100000/100000 [==============================] - 222s 2ms/step - loss: 0.6506 - acc: 0.7240 - val_loss: 0.7989 - val_acc: 0.6637\n",
      "Epoch 20/20\n",
      "100000/100000 [==============================] - 222s 2ms/step - loss: 0.6462 - acc: 0.7243 - val_loss: 0.7974 - val_acc: 0.6599\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23201fa44a8>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([x_train, x_train_left, x_train_right], y_train, epochs=20, verbose=1, batch_size=128, initial_epoch=10,\n",
    "          validation_data=([x_val, x_val_left, x_val_right], y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
