{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.engine.topology import Layer\n",
    "from keras import initializers as initializers, regularizers, constraints\n",
    "from keras.callbacks import Callback, ModelCheckpoint\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import Embedding, Input, Dense, CuDNNLSTM, LSTM, CuDNNGRU, GRU, Bidirectional, TimeDistributed, Dropout\n",
    "from keras import backend as K\n",
    "from keras import optimizers\n",
    "from keras.models import Model\n",
    "import nltk\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from nltk import tokenize\n",
    "import seaborn as sns\n",
    "\n",
    "from attention_with_context import AttentionWithContext\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 200000\n",
    "max_senten_len = 40\n",
    "max_senten_num = 6\n",
    "embed_size = 100\n",
    "VALIDATION_SPLIT = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total categories: 5\n",
      "stars\n",
      "1.0    14890\n",
      "2.0     8109\n",
      "3.0    11205\n",
      "4.0    21838\n",
      "5.0    43958\n",
      "dtype: int64\n",
      "total categories: 5\n",
      "stars\n",
      "1    1491\n",
      "2     777\n",
      "3    1114\n",
      "4    2212\n",
      "5    4406\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "TEXT_DATA_DIR = 'data'\n",
    "import os\n",
    "\n",
    "texts = []  # list of text samples\n",
    "labels = []  # list of labels\n",
    "files = ['train.csv', 'valid.csv']\n",
    "for file_name in files:\n",
    "    file = pd.read_csv(os.path.join(TEXT_DATA_DIR, file_name))\n",
    "    for line in file['text']:\n",
    "        texts.append(line)\n",
    "    for label in file['stars']:\n",
    "        labels.append(label)\n",
    "    cates = file.groupby('stars')\n",
    "\n",
    "    print(\"total categories:\", cates.ngroups)\n",
    "    print(cates.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "unknown_list = ['\\x7f','\\x80','\\x94','\\xa0','¡','¢','£','©','\\xad','®','¯','°','±','´','·','¹','½','¿','à','á','â','ã','ä','å','æ','ç','è','é','ê','ë','í','î','ñ','ó','ö','ø','ú','û','ü','ć','č','ē','ğ','ō','ş','ū','ə','ɛ','ɪ','ˈ','ˌ','ː','̀','́','ά','έ','ί','α','γ','δ','ε','η','ι','κ','λ','μ','ν','ξ','ο','ρ','ς','σ','τ','υ','ω','ό','ύ','ώ','а','б','г','е','к','л','м','о','с','т','у','ь','я','ِ','আ','ই','গ','ট','ভ','র','ল','স','া','ি','ো','্','ಠ','ᴥ','ᵒ','ᶅ','ᶘ','\\u2009','\\u200a','\\u200b','\\u200d','\\u200e','\\u200f','‐','–','—','―','‘','’','“','”','•','…','\\u2028','\\u202a','\\u202c','\\u202f','′','″','‹','›','€','™','≠','⊙','②','─','◕','☀','☃','☔','☕','☘','♂','♥','♫','♬','⚡','⚾','⛱','⛽','✂','✈','✊','✌','✔','✨','❄','❤','➡','ツ','象','️','︿','\\ufeff','🇦','🇨','🇫','🇬','🇭','🇵','🇷','🇸','🇺','🇿','🌈','🌊','🌍','🌎','🌑','🌒','🌓','🌔','🌕','🌖','🌗','🌘','🌝','🌞','🌤','🌦','🌮','🌳','🌴','🌷','🌸','🌹','🌿','🍁','🍂','🍃','🍅','🍆','🍋','🍌','🍍','🍎','🍑','🍒','🍓','🍔','🍕','🍖','🍗','🍜','🍝','🍞','🍩','🍪','🍭','🍰','🍴','🍷','🍸','🍹','🍻','🍽','🍾','🎀','🎁','🎂','🎃','🎄','🎅','🎆','🎈','🎉','🎊','🎓','🎤','🎮','🎵','🎶','🎾','🏀','🏃','🏅','🏆','🏈','🏊','🏨','🏫','🏰','🏼','🏽','🏾','🐊','🐋','🐍','🐎','🐔','🐘','🐛','🐝','🐭','🐮','🐱','🐲','🐶','🐷','🐸','🐾','👀','👅','👊','👋','👌','👍','👏','👑','👓','👖','👠','👩','👪','👬','👭','👯','👰','👴','👵','👶','👹','👻','👼','👽','💀','💁','💃','💄','💅','💇','💉','💊','💋','💍','💎','💐','💔','💕','💖','💗','💘','💙','💚','💛','💜','💞','💡','💤','💥','💦','💨','💩','💪','💯','💰','💸','💻','💼','📚','📝','📱','📷','📸','📺','🔋','🔍','🔑','🔥','🔪','🔮','🕵','🕺','🗣','😀','😁','😂','😃','😇','😈','😉','😊','😍','😎','😐','😑','😒','😔','😕','😘','😜','😞','😠','😡','😢','😩','😬','😭','😮','😰','😱','😳','😴','😵','😶','😷','😻','🙀','🙂','🙃','🙄','🙅','🙆','🙈','🙊','🙌','🙏','🚀','🚂','🚇','🚗','🚨','🚬','🚴','🛀','🤑','🤓','🤔','🤗','🤘','🤢','🤦','🤼','🥂','🥗','🦄']\n",
    "unknown_dict = {'\\x7f':\"\",'\\x80':\"euro\",'\\x94':\"\\\"\",'\\xa0':\" \",'¡':\"i\",'¢':\"cent\",'£':\"pound\",'©':\"copyright\",'\\xad':\" \",'®':\"registered\",'¯':\"\",'°':\"\",'±':\"\",'´':\"'\",'·':\"\",'¹':\"1\",'½':\"\",'¿':\"\",'à':\"\",'á':\"\",'â':\"\",'ã':\"\",'ä':\"\",'å':\"\",'æ':\"\",'ç':\"\",'è':\"\",'é':\"\",'ê':\"\",'ë':\"\",'í':\"\",'î':\"\",'ñ':\"\",'ó':\"\",'ö':\"\",'ø':\"\",'ú':\"\",'û':\"\",'ü':\"\",'ć':\"\",'č':\"\",'ē':\"\",'ğ':\"\",'ō':\"\",'ş':\"\",'ū':\"\",'ə':\"\",'ɛ':\"\",'ɪ':\"\",'ˈ':\"\",'ˌ':\"\",'ː':\"\",'̀':\"\",'́':\"\",'ά':\"\",'έ':\"\",'ί':\"\",'α':\"\",'γ':\"\",'δ':\"\",'ε':\"\",'η':\"\",'ι':\"\",'κ':\"\",'λ':\"\",'μ':\"\",'ν':\"\",'ξ':\"\",'ο':\"o\",'ρ':\"\",'ς':\"\",'σ':\"\",'τ':\"\",'υ':\"\",'ω':\"w\",'ό':\"o\",'ύ':\"u\",'ώ':\"w\",'а':\"a\",'б':\"\",'г':\"r\",'е':\"e\",'к':\"k\",'л':\"\",'м':\"m\",'о':\"o\",'с':\"c\",'т':\"t\",'у':\"y\",'ь':\"b\",'я':\"r\",'ِ':\"\",'আ':\"\",'ই':\"\",'গ':\"\",'ট':\"\",'ভ':\"\",'র':\"\",'ল':\"\",'স':\"\",'া':\"\",'ি':\"\",'ো':\"\",'্':\"\",'ಠ':\"\",'ᴥ':\"\",'ᵒ':\"\",'ᶅ':\"\",'ᶘ':\"\",'\\u2009':\" \",'\\u200a':\" \",'\\u200b':\" \",'\\u200d':\" \",'\\u200e':\" \",'\\u200f':\" \",'‐':\"-\",'–':\"-\",'—':\"-\",'―':\"-\",'‘':\"\\'\",'’':\"\\'\",'“':\"\\\"\",'”':\"\\\"\",'•':\" \",'…':\" \",'\\u2028':\" \",'\\u202a':\" \",'\\u202c':\" \",'\\u202f':\" \",'′':\"''\",'″':\"\\\"\",'‹':\"<\",'›':\">\",'€':\"\",'™':\"\",'≠':\"\",'⊙':\"\",'②':\"\",'─':\"-\",'◕':\"\",'☀':\"\",'☃':\"\",'☔':\"\",'☕':\"\",'☘':\"\",'♂':\"\",'♥':\"\",'♫':\"\",'♬':\"\",'⚡':\"\",'⚾':\"\",'⛱':\"\",'⛽':\"\",'✂':\"\",'✈':\"\",'✊':\"\",'✌':\"\",'✔':\"\",'✨':\"\",'❄':\"\",'❤':\"good\",'➡':\"\",'ツ':\"good\",'象':\"house\",'️':\"\",'︿':\"or\",'\\ufeff':\" \",'🇦':\"a\",'🇨':\"c\",'🇫':\"f\",'🇬':\"g\",'🇭':\"h\",'🇵':\"p\",'🇷':\"r\",'🇸':\"s\",'🇺':\"u\",'🇿':\"z\",'🌈':\"\",'🌊':\"\",'🌍':\"\",'🌎':\"\",'🌑':\"\",'🌒':\"\",'🌓':\"\",'🌔':\"\",'🌕':\"\",'🌖':\"\",'🌗':\"\",'🌘':\"\",'🌝':\"\",'🌞':\"\",'🌤':\"\",'🌦':\"\",'🌮':\"\",'🌳':\"\",'🌴':\"\",'🌷':\"\",'🌸':\"\",'🌹':\"\",'🌿':\"\",'🍁':\"\",'🍂':\"\",'🍃':\"\",'🍅':\"\",'🍆':\"\",'🍋':\"\",'🍌':\"\",'🍍':\"\",'🍎':\"\",'🍑':\"\",'🍒':\"\",'🍓':\"\",'🍔':\"\",'🍕':\"\",'🍖':\"\",'🍗':\"\",'🍜':\"\",'🍝':\"\",'🍞':\"\",'🍩':\"\",'🍪':\"\",'🍭':\"\",'🍰':\"\",'🍴':\"\",'🍷':\"\",'🍸':\"\",'🍹':\"\",'🍻':\"\",'🍽':\"\",'🍾':\"\",'🎀':\"\",'🎁':\"\",'🎂':\"\",'🎃':\"\",'🎄':\"\",'🎅':\"\",'🎆':\"\",'🎈':\"\",'🎉':\"\",'🎊':\"\",'🎓':\"\",'🎤':\"\",'🎮':\"\",'🎵':\"\",'🎶':\"\",'🎾':\"\",'🏀':\"\",'🏃':\"\",'🏅':\"\",'🏆':\"\",'🏈':\"\",'🏊':\"\",'🏨':\"\",'🏫':\"\",'🏰':\"\",'🏼':\"\",'🏽':\"\",'🏾':\"\",'🐊':\"\",'🐋':\"\",'🐍':\"\",'🐎':\"\",'🐔':\"\",'🐘':\"\",'🐛':\"\",'🐝':\"\",'🐭':\"\",'🐮':\"\",'🐱':\"\",'🐲':\"\",'🐶':\"\",'🐷':\"\",'🐸':\"\",'🐾':\"\",'👀':\"\",'👅':\"\",'👊':\"\",'👋':\"\",'👌':\"\",'👍':\"\",'👏':\"\",'👑':\"\",'👓':\"\",'👖':\"\",'👠':\"\",'👩':\"\",'👪':\"\",'👬':\"\",'👭':\"\",'👯':\"\",'👰':\"\",'👴':\"\",'👵':\"\",'👶':\"\",'👹':\"\",'👻':\"\",'👼':\"\",'👽':\"\",'💀':\"\",'💁':\"\",'💃':\"\",'💄':\"\",'💅':\"\",'💇':\"\",'💉':\"\",'💊':\"\",'💋':\"\",'💍':\"\",'💎':\"\",'💐':\"\",'💔':\"\",'💕':\"\",'💖':\"\",'💗':\"\",'💘':\"\",'💙':\"\",'💚':\"\",'💛':\"\",'💜':\"\",'💞':\"\",'💡':\"\",'💤':\"\",'💥':\"\",'💦':\"\",'💨':\"\",'💩':\"\",'💪':\"\",'💯':\"\",'💰':\"\",'💸':\"\",'💻':\"\",'💼':\"\",'📚':\"\",'📝':\"\",'📱':\"\",'📷':\"\",'📸':\"\",'📺':\"\",'🔋':\"\",'🔍':\"\",'🔑':\"\",'🔥':\"\",'🔪':\"\",'🔮':\"\",'🕵':\"\",'🕺':\"good\",'🗣':\"noisy\",'😀':\"good\",'😁':\"good\",'😂':\"good\",'😃':\"good\",'😇':\"good\",'😈':\"good\",'😉':\"good\",'😊':\"good\",'😍':\"good\",'😎':\"good\",'😐':\"bad\",'😑':\"bad\",'😒':\"bad\",'😔':\"bad\",'😕':\"bad\",'😘':\"good\",'😜':\"good\",'😞':\"bad\",'😠':\"bad\",'😡':\"bad\",'😢':\"no\",'😩':\"no\",'😬':\"no\",'😭':\"lol\",'😮':\"good\",'😰':\"sick\",'😱':\"maybe\",'😳':\"good\",'😴':\"sleepy\",'😵':\"maybe\",'😶':\"maybe\",'😷':\"sick\",'😻':\"good\",'🙀':\"wow\",'🙂':\"good\",'🙃':\"ok\",'🙄':\"so so\",'🙅':\"no\",'🙆':\"yes\",'🙈':\"good\",'🙊':\"maybe\",'🙌':\"soso\",'🙏':\"please\",'🚀':\"rocket\",'🚂':\"train\",'🚇':\"train\",'🚗':\"car\",'🚨':\"signal\",'🚬':\"smoke\",'🚴':\"cycle\",'🛀':\"clean\",'🤑':\"good\",'🤓':\"good\",'🤔':\"doubt\",'🤗':\"happy\",'🤘':\"good\",'🤢':\"bad\",'🤦':\"confused\",'🤼':\"happy\",'🥂':\"beer\",'🥗':\"salad\",'🦄':\"good\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "28000\n",
      "29000\n",
      "30000\n",
      "31000\n",
      "32000\n",
      "33000\n",
      "34000\n",
      "35000\n",
      "36000\n",
      "37000\n",
      "38000\n",
      "39000\n",
      "40000\n",
      "41000\n",
      "42000\n",
      "43000\n",
      "44000\n",
      "45000\n",
      "46000\n",
      "47000\n",
      "48000\n",
      "49000\n",
      "50000\n",
      "51000\n",
      "52000\n",
      "53000\n",
      "54000\n",
      "55000\n",
      "56000\n",
      "57000\n",
      "58000\n",
      "59000\n",
      "60000\n",
      "61000\n",
      "62000\n",
      "63000\n",
      "64000\n",
      "65000\n",
      "66000\n",
      "67000\n",
      "68000\n",
      "69000\n",
      "70000\n",
      "71000\n",
      "72000\n",
      "73000\n",
      "74000\n",
      "75000\n",
      "76000\n",
      "77000\n",
      "78000\n",
      "79000\n",
      "80000\n",
      "81000\n",
      "82000\n",
      "83000\n",
      "84000\n",
      "85000\n",
      "86000\n",
      "87000\n",
      "88000\n",
      "89000\n",
      "90000\n",
      "91000\n",
      "92000\n",
      "93000\n",
      "94000\n",
      "95000\n",
      "96000\n",
      "97000\n",
      "98000\n",
      "99000\n",
      "100000\n",
      "101000\n",
      "102000\n",
      "103000\n",
      "104000\n",
      "105000\n",
      "106000\n",
      "107000\n",
      "108000\n",
      "109000\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "# new_list = list()\n",
    "# count = 0\n",
    "# for line in texts:\n",
    "#     if count % 1000 == 0:\n",
    "#         print(count, end=\" \")\n",
    "#     new_line = \"\"\n",
    "#     for char in line:\n",
    "#         new_char = char\n",
    "#         if char in unknown_list:\n",
    "#             new_char = unknown_dict[char]\n",
    "#         line = line + new_char\n",
    "#     new_list.append(new_line)\n",
    "#     count = count + 1\n",
    "# print(\"DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gratisography\n",
      "— david\n",
      "the hamilton creator\n",
      "and its\n",
      "but if\n",
      "bob mcdonnell\n",
      "Total 70043 unique tokens.\n",
      "Shape of data tensor: (110000, 6, 40)\n",
      "Shape of labels tensor: (110000, 5)\n",
      "Number of positive and negative reviews in traing and validation set\n",
      "[1.0, 2.0, 3.0, 4.0, 5.0]\n",
      "[13025, 7163, 9916, 19164, 38732]\n",
      "[3356, 1723, 2403, 4886, 9632]\n"
     ]
    }
   ],
   "source": [
    "texts = new_list\n",
    "# df = shuffle(pd.read_json('data/News_Category_Dataset.json', lines=True)).reset_index()\n",
    "# df.category = df.category.map(lambda x: \"WORLDPOST\" if x == \"THE WORLDPOST\" else x)\n",
    "# df['text'] = df['headline'] + '. ' + df['short_description']\n",
    "# df = df[['text', 'category']]\n",
    "\n",
    "categories = np.array(labels)\n",
    "text = np.array(texts)\n",
    "\n",
    "\n",
    "import re\n",
    "\n",
    "\n",
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for dataset\n",
    "    Every dataset is lower cased except\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"\\\\\", \"\", string)\n",
    "    string = re.sub(r\"\\'\", \"\", string)\n",
    "    string = re.sub(r\"\\\"\", \"\", string)\n",
    "    return string.strip().lower()\n",
    "\n",
    "paras = []\n",
    "labels = []\n",
    "texts = []\n",
    "\n",
    "sent_lens = []\n",
    "sent_nums = []\n",
    "for idx in range(text.shape[0]):\n",
    "    text = clean_str(df.text[idx])\n",
    "    texts.append(text)\n",
    "    sentences = tokenize.sent_tokenize(text)\n",
    "    sent_nums.append(len(sentences))\n",
    "    for sent in sentences:\n",
    "        sent_lens.append(len(text_to_word_sequence(sent)))\n",
    "    paras.append(sentences)\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_features, oov_token=True)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "data = np.zeros((len(texts), max_senten_num, max_senten_len), dtype='int32')\n",
    "for i, sentences in enumerate(paras):\n",
    "    for j, sent in enumerate(sentences):\n",
    "        if j < max_senten_num:\n",
    "            wordTokens = text_to_word_sequence(sent)\n",
    "            k = 0\n",
    "            for _, word in enumerate(wordTokens):\n",
    "                try:\n",
    "                    if k < max_senten_len and tokenizer.word_index[word] < max_features:\n",
    "                        data[i, j, k] = tokenizer.word_index[word]\n",
    "                        k = k + 1\n",
    "                except:\n",
    "                    print(word)\n",
    "                    pass\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Total %s unique tokens.' % len(word_index))\n",
    "\n",
    "labels = pd.get_dummies(categories)\n",
    "\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of labels tensor:', labels.shape)\n",
    "\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels.iloc[indices]\n",
    "nb_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "\n",
    "x_train = data[:-nb_validation_samples]\n",
    "y_train = labels[:-nb_validation_samples]\n",
    "x_val = data[-nb_validation_samples:]\n",
    "y_val = labels[-nb_validation_samples:]\n",
    "print('Number of positive and negative reviews in traing and validation set')\n",
    "print(y_train.columns.tolist())\n",
    "print(y_train.sum(axis=0).tolist())\n",
    "print(y_val.sum(axis=0).tolist())\n",
    "\n",
    "REG_PARAM = 1e-13\n",
    "l2_reg = regularizers.l2(REG_PARAM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(88000, 6, 40)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 400000 word vectors.\n",
      "Total absent words are 16061 which is 22.93 % of total words\n"
     ]
    }
   ],
   "source": [
    "GLOVE_DIR = \"data/glove.6B.100d.txt\"\n",
    "embeddings_index = {}\n",
    "f = open(GLOVE_DIR, encoding='UTF-8')\n",
    "for line in f:\n",
    "    try:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    except:\n",
    "        print(word)\n",
    "        pass\n",
    "f.close()\n",
    "print('Total %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, embed_size))\n",
    "absent_words = 0\n",
    "absent_set = set()\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        for j in str(word):\n",
    "            absent_set.add(j)\n",
    "        absent_words += 1\n",
    "print('Total absent words are', absent_words, 'which is', \"%0.2f\" % (absent_words * 100 / len(word_index)),\n",
    "      '% of total words')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0',\n",
       " '1',\n",
       " '2',\n",
       " '3',\n",
       " '4',\n",
       " '5',\n",
       " '6',\n",
       " '7',\n",
       " '8',\n",
       " '9',\n",
       " 'T',\n",
       " 'a',\n",
       " 'b',\n",
       " 'c',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'g',\n",
       " 'h',\n",
       " 'i',\n",
       " 'j',\n",
       " 'k',\n",
       " 'l',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'p',\n",
       " 'q',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'v',\n",
       " 'w',\n",
       " 'x',\n",
       " 'y',\n",
       " 'z',\n",
       " '\\x7f',\n",
       " '\\x80',\n",
       " '\\x94',\n",
       " '\\xa0',\n",
       " '¡',\n",
       " '¢',\n",
       " '£',\n",
       " '©',\n",
       " '\\xad',\n",
       " '®',\n",
       " '¯',\n",
       " '°',\n",
       " '±',\n",
       " '´',\n",
       " '·',\n",
       " '¹',\n",
       " '½',\n",
       " '¿',\n",
       " 'à',\n",
       " 'á',\n",
       " 'â',\n",
       " 'ã',\n",
       " 'ä',\n",
       " 'å',\n",
       " 'æ',\n",
       " 'ç',\n",
       " 'è',\n",
       " 'é',\n",
       " 'ê',\n",
       " 'ë',\n",
       " 'í',\n",
       " 'î',\n",
       " 'ñ',\n",
       " 'ó',\n",
       " 'ö',\n",
       " 'ø',\n",
       " 'ú',\n",
       " 'û',\n",
       " 'ü',\n",
       " 'ć',\n",
       " 'č',\n",
       " 'ē',\n",
       " 'ğ',\n",
       " 'ō',\n",
       " 'ş',\n",
       " 'ū',\n",
       " 'ə',\n",
       " 'ɛ',\n",
       " 'ɪ',\n",
       " 'ˈ',\n",
       " 'ˌ',\n",
       " 'ː',\n",
       " '̀',\n",
       " '́',\n",
       " 'ά',\n",
       " 'έ',\n",
       " 'ί',\n",
       " 'α',\n",
       " 'γ',\n",
       " 'δ',\n",
       " 'ε',\n",
       " 'η',\n",
       " 'ι',\n",
       " 'κ',\n",
       " 'λ',\n",
       " 'μ',\n",
       " 'ν',\n",
       " 'ξ',\n",
       " 'ο',\n",
       " 'ρ',\n",
       " 'ς',\n",
       " 'σ',\n",
       " 'τ',\n",
       " 'υ',\n",
       " 'ω',\n",
       " 'ό',\n",
       " 'ύ',\n",
       " 'ώ',\n",
       " 'а',\n",
       " 'б',\n",
       " 'г',\n",
       " 'е',\n",
       " 'к',\n",
       " 'л',\n",
       " 'м',\n",
       " 'о',\n",
       " 'с',\n",
       " 'т',\n",
       " 'у',\n",
       " 'ь',\n",
       " 'я',\n",
       " 'ِ',\n",
       " 'আ',\n",
       " 'ই',\n",
       " 'গ',\n",
       " 'ট',\n",
       " 'ভ',\n",
       " 'র',\n",
       " 'ল',\n",
       " 'স',\n",
       " 'া',\n",
       " 'ি',\n",
       " 'ো',\n",
       " '্',\n",
       " 'ಠ',\n",
       " 'ᴥ',\n",
       " 'ᵒ',\n",
       " 'ᶅ',\n",
       " 'ᶘ',\n",
       " '\\u2009',\n",
       " '\\u200a',\n",
       " '\\u200b',\n",
       " '\\u200d',\n",
       " '\\u200e',\n",
       " '\\u200f',\n",
       " '‐',\n",
       " '–',\n",
       " '—',\n",
       " '―',\n",
       " '‘',\n",
       " '’',\n",
       " '“',\n",
       " '”',\n",
       " '•',\n",
       " '…',\n",
       " '\\u2028',\n",
       " '\\u202a',\n",
       " '\\u202c',\n",
       " '\\u202f',\n",
       " '′',\n",
       " '″',\n",
       " '‹',\n",
       " '›',\n",
       " '€',\n",
       " '™',\n",
       " '≠',\n",
       " '⊙',\n",
       " '②',\n",
       " '─',\n",
       " '◕',\n",
       " '☀',\n",
       " '☃',\n",
       " '☔',\n",
       " '☕',\n",
       " '☘',\n",
       " '♂',\n",
       " '♥',\n",
       " '♫',\n",
       " '♬',\n",
       " '⚡',\n",
       " '⚾',\n",
       " '⛱',\n",
       " '⛽',\n",
       " '✂',\n",
       " '✈',\n",
       " '✊',\n",
       " '✌',\n",
       " '✔',\n",
       " '✨',\n",
       " '❄',\n",
       " '❤',\n",
       " '➡',\n",
       " 'ツ',\n",
       " '象',\n",
       " '️',\n",
       " '︿',\n",
       " '\\ufeff',\n",
       " '🇦',\n",
       " '🇨',\n",
       " '🇫',\n",
       " '🇬',\n",
       " '🇭',\n",
       " '🇵',\n",
       " '🇷',\n",
       " '🇸',\n",
       " '🇺',\n",
       " '🇿',\n",
       " '🌈',\n",
       " '🌊',\n",
       " '🌍',\n",
       " '🌎',\n",
       " '🌑',\n",
       " '🌒',\n",
       " '🌓',\n",
       " '🌔',\n",
       " '🌕',\n",
       " '🌖',\n",
       " '🌗',\n",
       " '🌘',\n",
       " '🌝',\n",
       " '🌞',\n",
       " '🌤',\n",
       " '🌦',\n",
       " '🌮',\n",
       " '🌳',\n",
       " '🌴',\n",
       " '🌷',\n",
       " '🌸',\n",
       " '🌹',\n",
       " '🌿',\n",
       " '🍁',\n",
       " '🍂',\n",
       " '🍃',\n",
       " '🍅',\n",
       " '🍆',\n",
       " '🍋',\n",
       " '🍌',\n",
       " '🍍',\n",
       " '🍎',\n",
       " '🍑',\n",
       " '🍒',\n",
       " '🍓',\n",
       " '🍔',\n",
       " '🍕',\n",
       " '🍖',\n",
       " '🍗',\n",
       " '🍜',\n",
       " '🍝',\n",
       " '🍞',\n",
       " '🍩',\n",
       " '🍪',\n",
       " '🍭',\n",
       " '🍰',\n",
       " '🍴',\n",
       " '🍷',\n",
       " '🍸',\n",
       " '🍹',\n",
       " '🍻',\n",
       " '🍽',\n",
       " '🍾',\n",
       " '🎀',\n",
       " '🎁',\n",
       " '🎂',\n",
       " '🎃',\n",
       " '🎄',\n",
       " '🎅',\n",
       " '🎆',\n",
       " '🎈',\n",
       " '🎉',\n",
       " '🎊',\n",
       " '🎓',\n",
       " '🎤',\n",
       " '🎮',\n",
       " '🎵',\n",
       " '🎶',\n",
       " '🎾',\n",
       " '🏀',\n",
       " '🏃',\n",
       " '🏅',\n",
       " '🏆',\n",
       " '🏈',\n",
       " '🏊',\n",
       " '🏨',\n",
       " '🏫',\n",
       " '🏰',\n",
       " '🏼',\n",
       " '🏽',\n",
       " '🏾',\n",
       " '🐊',\n",
       " '🐋',\n",
       " '🐍',\n",
       " '🐎',\n",
       " '🐔',\n",
       " '🐘',\n",
       " '🐛',\n",
       " '🐝',\n",
       " '🐭',\n",
       " '🐮',\n",
       " '🐱',\n",
       " '🐲',\n",
       " '🐶',\n",
       " '🐷',\n",
       " '🐸',\n",
       " '🐾',\n",
       " '👀',\n",
       " '👅',\n",
       " '👊',\n",
       " '👋',\n",
       " '👌',\n",
       " '👍',\n",
       " '👏',\n",
       " '👑',\n",
       " '👓',\n",
       " '👖',\n",
       " '👠',\n",
       " '👩',\n",
       " '👪',\n",
       " '👬',\n",
       " '👭',\n",
       " '👯',\n",
       " '👰',\n",
       " '👴',\n",
       " '👵',\n",
       " '👶',\n",
       " '👹',\n",
       " '👻',\n",
       " '👼',\n",
       " '👽',\n",
       " '💀',\n",
       " '💁',\n",
       " '💃',\n",
       " '💄',\n",
       " '💅',\n",
       " '💇',\n",
       " '💉',\n",
       " '💊',\n",
       " '💋',\n",
       " '💍',\n",
       " '💎',\n",
       " '💐',\n",
       " '💔',\n",
       " '💕',\n",
       " '💖',\n",
       " '💗',\n",
       " '💘',\n",
       " '💙',\n",
       " '💚',\n",
       " '💛',\n",
       " '💜',\n",
       " '💞',\n",
       " '💡',\n",
       " '💤',\n",
       " '💥',\n",
       " '💦',\n",
       " '💨',\n",
       " '💩',\n",
       " '💪',\n",
       " '💯',\n",
       " '💰',\n",
       " '💸',\n",
       " '💻',\n",
       " '💼',\n",
       " '📚',\n",
       " '📝',\n",
       " '📱',\n",
       " '📷',\n",
       " '📸',\n",
       " '📺',\n",
       " '🔋',\n",
       " '🔍',\n",
       " '🔑',\n",
       " '🔥',\n",
       " '🔪',\n",
       " '🔮',\n",
       " '🕵',\n",
       " '🕺',\n",
       " '🗣',\n",
       " '😀',\n",
       " '😁',\n",
       " '😂',\n",
       " '😃',\n",
       " '😇',\n",
       " '😈',\n",
       " '😉',\n",
       " '😊',\n",
       " '😍',\n",
       " '😎',\n",
       " '😐',\n",
       " '😑',\n",
       " '😒',\n",
       " '😔',\n",
       " '😕',\n",
       " '😘',\n",
       " '😜',\n",
       " '😞',\n",
       " '😠',\n",
       " '😡',\n",
       " '😢',\n",
       " '😩',\n",
       " '😬',\n",
       " '😭',\n",
       " '😮',\n",
       " '😰',\n",
       " '😱',\n",
       " '😳',\n",
       " '😴',\n",
       " '😵',\n",
       " '😶',\n",
       " '😷',\n",
       " '😻',\n",
       " '🙀',\n",
       " '🙂',\n",
       " '🙃',\n",
       " '🙄',\n",
       " '🙅',\n",
       " '🙆',\n",
       " '🙈',\n",
       " '🙊',\n",
       " '🙌',\n",
       " '🙏',\n",
       " '🚀',\n",
       " '🚂',\n",
       " '🚇',\n",
       " '🚗',\n",
       " '🚨',\n",
       " '🚬',\n",
       " '🚴',\n",
       " '🛀',\n",
       " '🤑',\n",
       " '🤓',\n",
       " '🤔',\n",
       " '🤗',\n",
       " '🤘',\n",
       " '🤢',\n",
       " '🤦',\n",
       " '🤼',\n",
       " '🥂',\n",
       " '🥗',\n",
       " '🦄'}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "absent_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 88000 samples, validate on 22000 samples\n",
      "Epoch 1/50\n",
      "47104/88000 [===============>..............] - ETA: 30s - loss: 1.4430 - acc: 0.4343"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-44-83d218f0caee>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[0mcheckpoint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mModelCheckpoint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'best_model.h5'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmonitor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'val_loss'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_best_only\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'auto'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m512\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "embedding_layer = Embedding(len(word_index) + 1, embed_size, weights=[embedding_matrix], input_length=max_senten_len,\n",
    "                            trainable=False)\n",
    "\n",
    "word_input = Input(shape=(max_senten_len,), dtype='float32')\n",
    "word_sequences = embedding_layer(word_input)\n",
    "word_lstm = Bidirectional(CuDNNLSTM(150, return_sequences=True, kernel_regularizer=l2_reg))(word_sequences)\n",
    "word_dense = TimeDistributed(Dense(200, kernel_regularizer=l2_reg))(word_lstm)\n",
    "word_att = AttentionWithContext()(word_dense)\n",
    "wordEncoder = Model(word_input, word_att)\n",
    "\n",
    "sent_input = Input(shape=(max_senten_num, max_senten_len), dtype='float32')\n",
    "sent_encoder = TimeDistributed(wordEncoder)(sent_input)\n",
    "sent_lstm = Bidirectional(CuDNNLSTM(150, return_sequences=True, kernel_regularizer=l2_reg))(sent_encoder)\n",
    "sent_dense = TimeDistributed(Dense(200, kernel_regularizer=l2_reg))(sent_lstm)\n",
    "sent_att = Dropout(0.5)(AttentionWithContext()(sent_dense))\n",
    "preds = Dense(5, activation='softmax')(sent_att)\n",
    "model = Model(sent_input, preds)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "\n",
    "checkpoint = ModelCheckpoint('best_model.h5', verbose=0, monitor='val_loss', save_best_only=True, mode='auto')\n",
    "\n",
    "history = model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=50, batch_size=512, callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
