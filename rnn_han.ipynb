{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.engine.topology import Layer\n",
    "from keras import initializers as initializers, regularizers, constraints\n",
    "from keras.callbacks import Callback, ModelCheckpoint\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import Embedding, Input, Dense, CuDNNLSTM, LSTM, CuDNNGRU, GRU, Bidirectional, TimeDistributed, Dropout\n",
    "from keras import backend as K\n",
    "from keras import optimizers\n",
    "from keras.models import Model\n",
    "import nltk\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from nltk import tokenize\n",
    "import seaborn as sns\n",
    "\n",
    "from attention_with_context import AttentionWithContext\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 200000\n",
    "max_senten_len = 40\n",
    "max_senten_num = 6\n",
    "embed_size = 100\n",
    "VALIDATION_SPLIT = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total categories: 5\n",
      "stars\n",
      "1.0    14890\n",
      "2.0     8109\n",
      "3.0    11205\n",
      "4.0    21838\n",
      "5.0    43958\n",
      "dtype: int64\n",
      "total categories: 5\n",
      "stars\n",
      "1    1491\n",
      "2     777\n",
      "3    1114\n",
      "4    2212\n",
      "5    4406\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "TEXT_DATA_DIR = 'data'\n",
    "import os\n",
    "\n",
    "texts = []  # list of text samples\n",
    "labels = []  # list of labels\n",
    "files = ['train.csv', 'valid.csv']\n",
    "for file_name in files:\n",
    "    file = pd.read_csv(os.path.join(TEXT_DATA_DIR, file_name))\n",
    "    for line in file['text']:\n",
    "        texts.append(line)\n",
    "    for label in file['stars']:\n",
    "        labels.append(label)\n",
    "    cates = file.groupby('stars')\n",
    "\n",
    "    print(\"total categories:\", cates.ngroups)\n",
    "    print(cates.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "unknown_list = ['\\x7f','\\x80','\\x94','\\xa0','Â¡','Â¢','Â£','Â©','\\xad','Â®','Â¯','Â°','Â±','Â´','Â·','Â¹','Â½','Â¿','Ã ','Ã¡','Ã¢','Ã£','Ã¤','Ã¥','Ã¦','Ã§','Ã¨','Ã©','Ãª','Ã«','Ã­','Ã®','Ã±','Ã³','Ã¶','Ã¸','Ãº','Ã»','Ã¼','Ä‡','Ä','Ä“','ÄŸ','Å','ÅŸ','Å«','É™','É›','Éª','Ëˆ','ËŒ','Ë','Ì€','Ì','Î¬','Î­','Î¯','Î±','Î³','Î´','Îµ','Î·','Î¹','Îº','Î»','Î¼','Î½','Î¾','Î¿','Ï','Ï‚','Ïƒ','Ï„','Ï…','Ï‰','ÏŒ','Ï','Ï','Ğ°','Ğ±','Ğ³','Ğµ','Ğº','Ğ»','Ğ¼','Ğ¾','Ñ','Ñ‚','Ñƒ','ÑŒ','Ñ','Ù','à¦†','à¦‡','à¦—','à¦Ÿ','à¦­','à¦°','à¦²','à¦¸','à¦¾','à¦¿','à§‹','à§','à² ','á´¥','áµ’','á¶…','á¶˜','\\u2009','\\u200a','\\u200b','\\u200d','\\u200e','\\u200f','â€','â€“','â€”','â€•','â€˜','â€™','â€œ','â€','â€¢','â€¦','\\u2028','\\u202a','\\u202c','\\u202f','â€²','â€³','â€¹','â€º','â‚¬','â„¢','â‰ ','âŠ™','â‘¡','â”€','â—•','â˜€','â˜ƒ','â˜”','â˜•','â˜˜','â™‚','â™¥','â™«','â™¬','âš¡','âš¾','â›±','â›½','âœ‚','âœˆ','âœŠ','âœŒ','âœ”','âœ¨','â„','â¤','â¡','ãƒ„','è±¡','ï¸','ï¸¿','\\ufeff','ğŸ‡¦','ğŸ‡¨','ğŸ‡«','ğŸ‡¬','ğŸ‡­','ğŸ‡µ','ğŸ‡·','ğŸ‡¸','ğŸ‡º','ğŸ‡¿','ğŸŒˆ','ğŸŒŠ','ğŸŒ','ğŸŒ','ğŸŒ‘','ğŸŒ’','ğŸŒ“','ğŸŒ”','ğŸŒ•','ğŸŒ–','ğŸŒ—','ğŸŒ˜','ğŸŒ','ğŸŒ','ğŸŒ¤','ğŸŒ¦','ğŸŒ®','ğŸŒ³','ğŸŒ´','ğŸŒ·','ğŸŒ¸','ğŸŒ¹','ğŸŒ¿','ğŸ','ğŸ‚','ğŸƒ','ğŸ…','ğŸ†','ğŸ‹','ğŸŒ','ğŸ','ğŸ','ğŸ‘','ğŸ’','ğŸ“','ğŸ”','ğŸ•','ğŸ–','ğŸ—','ğŸœ','ğŸ','ğŸ','ğŸ©','ğŸª','ğŸ­','ğŸ°','ğŸ´','ğŸ·','ğŸ¸','ğŸ¹','ğŸ»','ğŸ½','ğŸ¾','ğŸ€','ğŸ','ğŸ‚','ğŸƒ','ğŸ„','ğŸ…','ğŸ†','ğŸˆ','ğŸ‰','ğŸŠ','ğŸ“','ğŸ¤','ğŸ®','ğŸµ','ğŸ¶','ğŸ¾','ğŸ€','ğŸƒ','ğŸ…','ğŸ†','ğŸˆ','ğŸŠ','ğŸ¨','ğŸ«','ğŸ°','ğŸ¼','ğŸ½','ğŸ¾','ğŸŠ','ğŸ‹','ğŸ','ğŸ','ğŸ”','ğŸ˜','ğŸ›','ğŸ','ğŸ­','ğŸ®','ğŸ±','ğŸ²','ğŸ¶','ğŸ·','ğŸ¸','ğŸ¾','ğŸ‘€','ğŸ‘…','ğŸ‘Š','ğŸ‘‹','ğŸ‘Œ','ğŸ‘','ğŸ‘','ğŸ‘‘','ğŸ‘“','ğŸ‘–','ğŸ‘ ','ğŸ‘©','ğŸ‘ª','ğŸ‘¬','ğŸ‘­','ğŸ‘¯','ğŸ‘°','ğŸ‘´','ğŸ‘µ','ğŸ‘¶','ğŸ‘¹','ğŸ‘»','ğŸ‘¼','ğŸ‘½','ğŸ’€','ğŸ’','ğŸ’ƒ','ğŸ’„','ğŸ’…','ğŸ’‡','ğŸ’‰','ğŸ’Š','ğŸ’‹','ğŸ’','ğŸ’','ğŸ’','ğŸ’”','ğŸ’•','ğŸ’–','ğŸ’—','ğŸ’˜','ğŸ’™','ğŸ’š','ğŸ’›','ğŸ’œ','ğŸ’','ğŸ’¡','ğŸ’¤','ğŸ’¥','ğŸ’¦','ğŸ’¨','ğŸ’©','ğŸ’ª','ğŸ’¯','ğŸ’°','ğŸ’¸','ğŸ’»','ğŸ’¼','ğŸ“š','ğŸ“','ğŸ“±','ğŸ“·','ğŸ“¸','ğŸ“º','ğŸ”‹','ğŸ”','ğŸ”‘','ğŸ”¥','ğŸ”ª','ğŸ”®','ğŸ•µ','ğŸ•º','ğŸ—£','ğŸ˜€','ğŸ˜','ğŸ˜‚','ğŸ˜ƒ','ğŸ˜‡','ğŸ˜ˆ','ğŸ˜‰','ğŸ˜Š','ğŸ˜','ğŸ˜','ğŸ˜','ğŸ˜‘','ğŸ˜’','ğŸ˜”','ğŸ˜•','ğŸ˜˜','ğŸ˜œ','ğŸ˜','ğŸ˜ ','ğŸ˜¡','ğŸ˜¢','ğŸ˜©','ğŸ˜¬','ğŸ˜­','ğŸ˜®','ğŸ˜°','ğŸ˜±','ğŸ˜³','ğŸ˜´','ğŸ˜µ','ğŸ˜¶','ğŸ˜·','ğŸ˜»','ğŸ™€','ğŸ™‚','ğŸ™ƒ','ğŸ™„','ğŸ™…','ğŸ™†','ğŸ™ˆ','ğŸ™Š','ğŸ™Œ','ğŸ™','ğŸš€','ğŸš‚','ğŸš‡','ğŸš—','ğŸš¨','ğŸš¬','ğŸš´','ğŸ›€','ğŸ¤‘','ğŸ¤“','ğŸ¤”','ğŸ¤—','ğŸ¤˜','ğŸ¤¢','ğŸ¤¦','ğŸ¤¼','ğŸ¥‚','ğŸ¥—','ğŸ¦„']\n",
    "unknown_dict = {'\\x7f':\"\",'\\x80':\"euro\",'\\x94':\"\\\"\",'\\xa0':\" \",'Â¡':\"i\",'Â¢':\"cent\",'Â£':\"pound\",'Â©':\"copyright\",'\\xad':\" \",'Â®':\"registered\",'Â¯':\"\",'Â°':\"\",'Â±':\"\",'Â´':\"'\",'Â·':\"\",'Â¹':\"1\",'Â½':\"\",'Â¿':\"\",'Ã ':\"\",'Ã¡':\"\",'Ã¢':\"\",'Ã£':\"\",'Ã¤':\"\",'Ã¥':\"\",'Ã¦':\"\",'Ã§':\"\",'Ã¨':\"\",'Ã©':\"\",'Ãª':\"\",'Ã«':\"\",'Ã­':\"\",'Ã®':\"\",'Ã±':\"\",'Ã³':\"\",'Ã¶':\"\",'Ã¸':\"\",'Ãº':\"\",'Ã»':\"\",'Ã¼':\"\",'Ä‡':\"\",'Ä':\"\",'Ä“':\"\",'ÄŸ':\"\",'Å':\"\",'ÅŸ':\"\",'Å«':\"\",'É™':\"\",'É›':\"\",'Éª':\"\",'Ëˆ':\"\",'ËŒ':\"\",'Ë':\"\",'Ì€':\"\",'Ì':\"\",'Î¬':\"\",'Î­':\"\",'Î¯':\"\",'Î±':\"\",'Î³':\"\",'Î´':\"\",'Îµ':\"\",'Î·':\"\",'Î¹':\"\",'Îº':\"\",'Î»':\"\",'Î¼':\"\",'Î½':\"\",'Î¾':\"\",'Î¿':\"o\",'Ï':\"\",'Ï‚':\"\",'Ïƒ':\"\",'Ï„':\"\",'Ï…':\"\",'Ï‰':\"w\",'ÏŒ':\"o\",'Ï':\"u\",'Ï':\"w\",'Ğ°':\"a\",'Ğ±':\"\",'Ğ³':\"r\",'Ğµ':\"e\",'Ğº':\"k\",'Ğ»':\"\",'Ğ¼':\"m\",'Ğ¾':\"o\",'Ñ':\"c\",'Ñ‚':\"t\",'Ñƒ':\"y\",'ÑŒ':\"b\",'Ñ':\"r\",'Ù':\"\",'à¦†':\"\",'à¦‡':\"\",'à¦—':\"\",'à¦Ÿ':\"\",'à¦­':\"\",'à¦°':\"\",'à¦²':\"\",'à¦¸':\"\",'à¦¾':\"\",'à¦¿':\"\",'à§‹':\"\",'à§':\"\",'à² ':\"\",'á´¥':\"\",'áµ’':\"\",'á¶…':\"\",'á¶˜':\"\",'\\u2009':\" \",'\\u200a':\" \",'\\u200b':\" \",'\\u200d':\" \",'\\u200e':\" \",'\\u200f':\" \",'â€':\"-\",'â€“':\"-\",'â€”':\"-\",'â€•':\"-\",'â€˜':\"\\'\",'â€™':\"\\'\",'â€œ':\"\\\"\",'â€':\"\\\"\",'â€¢':\" \",'â€¦':\" \",'\\u2028':\" \",'\\u202a':\" \",'\\u202c':\" \",'\\u202f':\" \",'â€²':\"''\",'â€³':\"\\\"\",'â€¹':\"<\",'â€º':\">\",'â‚¬':\"\",'â„¢':\"\",'â‰ ':\"\",'âŠ™':\"\",'â‘¡':\"\",'â”€':\"-\",'â—•':\"\",'â˜€':\"\",'â˜ƒ':\"\",'â˜”':\"\",'â˜•':\"\",'â˜˜':\"\",'â™‚':\"\",'â™¥':\"\",'â™«':\"\",'â™¬':\"\",'âš¡':\"\",'âš¾':\"\",'â›±':\"\",'â›½':\"\",'âœ‚':\"\",'âœˆ':\"\",'âœŠ':\"\",'âœŒ':\"\",'âœ”':\"\",'âœ¨':\"\",'â„':\"\",'â¤':\"good\",'â¡':\"\",'ãƒ„':\"good\",'è±¡':\"house\",'ï¸':\"\",'ï¸¿':\"or\",'\\ufeff':\" \",'ğŸ‡¦':\"a\",'ğŸ‡¨':\"c\",'ğŸ‡«':\"f\",'ğŸ‡¬':\"g\",'ğŸ‡­':\"h\",'ğŸ‡µ':\"p\",'ğŸ‡·':\"r\",'ğŸ‡¸':\"s\",'ğŸ‡º':\"u\",'ğŸ‡¿':\"z\",'ğŸŒˆ':\"\",'ğŸŒŠ':\"\",'ğŸŒ':\"\",'ğŸŒ':\"\",'ğŸŒ‘':\"\",'ğŸŒ’':\"\",'ğŸŒ“':\"\",'ğŸŒ”':\"\",'ğŸŒ•':\"\",'ğŸŒ–':\"\",'ğŸŒ—':\"\",'ğŸŒ˜':\"\",'ğŸŒ':\"\",'ğŸŒ':\"\",'ğŸŒ¤':\"\",'ğŸŒ¦':\"\",'ğŸŒ®':\"\",'ğŸŒ³':\"\",'ğŸŒ´':\"\",'ğŸŒ·':\"\",'ğŸŒ¸':\"\",'ğŸŒ¹':\"\",'ğŸŒ¿':\"\",'ğŸ':\"\",'ğŸ‚':\"\",'ğŸƒ':\"\",'ğŸ…':\"\",'ğŸ†':\"\",'ğŸ‹':\"\",'ğŸŒ':\"\",'ğŸ':\"\",'ğŸ':\"\",'ğŸ‘':\"\",'ğŸ’':\"\",'ğŸ“':\"\",'ğŸ”':\"\",'ğŸ•':\"\",'ğŸ–':\"\",'ğŸ—':\"\",'ğŸœ':\"\",'ğŸ':\"\",'ğŸ':\"\",'ğŸ©':\"\",'ğŸª':\"\",'ğŸ­':\"\",'ğŸ°':\"\",'ğŸ´':\"\",'ğŸ·':\"\",'ğŸ¸':\"\",'ğŸ¹':\"\",'ğŸ»':\"\",'ğŸ½':\"\",'ğŸ¾':\"\",'ğŸ€':\"\",'ğŸ':\"\",'ğŸ‚':\"\",'ğŸƒ':\"\",'ğŸ„':\"\",'ğŸ…':\"\",'ğŸ†':\"\",'ğŸˆ':\"\",'ğŸ‰':\"\",'ğŸŠ':\"\",'ğŸ“':\"\",'ğŸ¤':\"\",'ğŸ®':\"\",'ğŸµ':\"\",'ğŸ¶':\"\",'ğŸ¾':\"\",'ğŸ€':\"\",'ğŸƒ':\"\",'ğŸ…':\"\",'ğŸ†':\"\",'ğŸˆ':\"\",'ğŸŠ':\"\",'ğŸ¨':\"\",'ğŸ«':\"\",'ğŸ°':\"\",'ğŸ¼':\"\",'ğŸ½':\"\",'ğŸ¾':\"\",'ğŸŠ':\"\",'ğŸ‹':\"\",'ğŸ':\"\",'ğŸ':\"\",'ğŸ”':\"\",'ğŸ˜':\"\",'ğŸ›':\"\",'ğŸ':\"\",'ğŸ­':\"\",'ğŸ®':\"\",'ğŸ±':\"\",'ğŸ²':\"\",'ğŸ¶':\"\",'ğŸ·':\"\",'ğŸ¸':\"\",'ğŸ¾':\"\",'ğŸ‘€':\"\",'ğŸ‘…':\"\",'ğŸ‘Š':\"\",'ğŸ‘‹':\"\",'ğŸ‘Œ':\"\",'ğŸ‘':\"\",'ğŸ‘':\"\",'ğŸ‘‘':\"\",'ğŸ‘“':\"\",'ğŸ‘–':\"\",'ğŸ‘ ':\"\",'ğŸ‘©':\"\",'ğŸ‘ª':\"\",'ğŸ‘¬':\"\",'ğŸ‘­':\"\",'ğŸ‘¯':\"\",'ğŸ‘°':\"\",'ğŸ‘´':\"\",'ğŸ‘µ':\"\",'ğŸ‘¶':\"\",'ğŸ‘¹':\"\",'ğŸ‘»':\"\",'ğŸ‘¼':\"\",'ğŸ‘½':\"\",'ğŸ’€':\"\",'ğŸ’':\"\",'ğŸ’ƒ':\"\",'ğŸ’„':\"\",'ğŸ’…':\"\",'ğŸ’‡':\"\",'ğŸ’‰':\"\",'ğŸ’Š':\"\",'ğŸ’‹':\"\",'ğŸ’':\"\",'ğŸ’':\"\",'ğŸ’':\"\",'ğŸ’”':\"\",'ğŸ’•':\"\",'ğŸ’–':\"\",'ğŸ’—':\"\",'ğŸ’˜':\"\",'ğŸ’™':\"\",'ğŸ’š':\"\",'ğŸ’›':\"\",'ğŸ’œ':\"\",'ğŸ’':\"\",'ğŸ’¡':\"\",'ğŸ’¤':\"\",'ğŸ’¥':\"\",'ğŸ’¦':\"\",'ğŸ’¨':\"\",'ğŸ’©':\"\",'ğŸ’ª':\"\",'ğŸ’¯':\"\",'ğŸ’°':\"\",'ğŸ’¸':\"\",'ğŸ’»':\"\",'ğŸ’¼':\"\",'ğŸ“š':\"\",'ğŸ“':\"\",'ğŸ“±':\"\",'ğŸ“·':\"\",'ğŸ“¸':\"\",'ğŸ“º':\"\",'ğŸ”‹':\"\",'ğŸ”':\"\",'ğŸ”‘':\"\",'ğŸ”¥':\"\",'ğŸ”ª':\"\",'ğŸ”®':\"\",'ğŸ•µ':\"\",'ğŸ•º':\"good\",'ğŸ—£':\"noisy\",'ğŸ˜€':\"good\",'ğŸ˜':\"good\",'ğŸ˜‚':\"good\",'ğŸ˜ƒ':\"good\",'ğŸ˜‡':\"good\",'ğŸ˜ˆ':\"good\",'ğŸ˜‰':\"good\",'ğŸ˜Š':\"good\",'ğŸ˜':\"good\",'ğŸ˜':\"good\",'ğŸ˜':\"bad\",'ğŸ˜‘':\"bad\",'ğŸ˜’':\"bad\",'ğŸ˜”':\"bad\",'ğŸ˜•':\"bad\",'ğŸ˜˜':\"good\",'ğŸ˜œ':\"good\",'ğŸ˜':\"bad\",'ğŸ˜ ':\"bad\",'ğŸ˜¡':\"bad\",'ğŸ˜¢':\"no\",'ğŸ˜©':\"no\",'ğŸ˜¬':\"no\",'ğŸ˜­':\"lol\",'ğŸ˜®':\"good\",'ğŸ˜°':\"sick\",'ğŸ˜±':\"maybe\",'ğŸ˜³':\"good\",'ğŸ˜´':\"sleepy\",'ğŸ˜µ':\"maybe\",'ğŸ˜¶':\"maybe\",'ğŸ˜·':\"sick\",'ğŸ˜»':\"good\",'ğŸ™€':\"wow\",'ğŸ™‚':\"good\",'ğŸ™ƒ':\"ok\",'ğŸ™„':\"so so\",'ğŸ™…':\"no\",'ğŸ™†':\"yes\",'ğŸ™ˆ':\"good\",'ğŸ™Š':\"maybe\",'ğŸ™Œ':\"soso\",'ğŸ™':\"please\",'ğŸš€':\"rocket\",'ğŸš‚':\"train\",'ğŸš‡':\"train\",'ğŸš—':\"car\",'ğŸš¨':\"signal\",'ğŸš¬':\"smoke\",'ğŸš´':\"cycle\",'ğŸ›€':\"clean\",'ğŸ¤‘':\"good\",'ğŸ¤“':\"good\",'ğŸ¤”':\"doubt\",'ğŸ¤—':\"happy\",'ğŸ¤˜':\"good\",'ğŸ¤¢':\"bad\",'ğŸ¤¦':\"confused\",'ğŸ¤¼':\"happy\",'ğŸ¥‚':\"beer\",'ğŸ¥—':\"salad\",'ğŸ¦„':\"good\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "28000\n",
      "29000\n",
      "30000\n",
      "31000\n",
      "32000\n",
      "33000\n",
      "34000\n",
      "35000\n",
      "36000\n",
      "37000\n",
      "38000\n",
      "39000\n",
      "40000\n",
      "41000\n",
      "42000\n",
      "43000\n",
      "44000\n",
      "45000\n",
      "46000\n",
      "47000\n",
      "48000\n",
      "49000\n",
      "50000\n",
      "51000\n",
      "52000\n",
      "53000\n",
      "54000\n",
      "55000\n",
      "56000\n",
      "57000\n",
      "58000\n",
      "59000\n",
      "60000\n",
      "61000\n",
      "62000\n",
      "63000\n",
      "64000\n",
      "65000\n",
      "66000\n",
      "67000\n",
      "68000\n",
      "69000\n",
      "70000\n",
      "71000\n",
      "72000\n",
      "73000\n",
      "74000\n",
      "75000\n",
      "76000\n",
      "77000\n",
      "78000\n",
      "79000\n",
      "80000\n",
      "81000\n",
      "82000\n",
      "83000\n",
      "84000\n",
      "85000\n",
      "86000\n",
      "87000\n",
      "88000\n",
      "89000\n",
      "90000\n",
      "91000\n",
      "92000\n",
      "93000\n",
      "94000\n",
      "95000\n",
      "96000\n",
      "97000\n",
      "98000\n",
      "99000\n",
      "100000\n",
      "101000\n",
      "102000\n",
      "103000\n",
      "104000\n",
      "105000\n",
      "106000\n",
      "107000\n",
      "108000\n",
      "109000\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "# new_list = list()\n",
    "# count = 0\n",
    "# for line in texts:\n",
    "#     if count % 1000 == 0:\n",
    "#         print(count, end=\" \")\n",
    "#     new_line = \"\"\n",
    "#     for char in line:\n",
    "#         new_char = char\n",
    "#         if char in unknown_list:\n",
    "#             new_char = unknown_dict[char]\n",
    "#         line = line + new_char\n",
    "#     new_list.append(new_line)\n",
    "#     count = count + 1\n",
    "# print(\"DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gratisography\n",
      "â€”â€Šdavid\n",
      "theÂ hamiltonÂ creator\n",
      "andÂ its\n",
      "butÂ if\n",
      "bobÂ mcdonnell\n",
      "Total 70043 unique tokens.\n",
      "Shape of data tensor: (110000, 6, 40)\n",
      "Shape of labels tensor: (110000, 5)\n",
      "Number of positive and negative reviews in traing and validation set\n",
      "[1.0, 2.0, 3.0, 4.0, 5.0]\n",
      "[13025, 7163, 9916, 19164, 38732]\n",
      "[3356, 1723, 2403, 4886, 9632]\n"
     ]
    }
   ],
   "source": [
    "texts = new_list\n",
    "# df = shuffle(pd.read_json('data/News_Category_Dataset.json', lines=True)).reset_index()\n",
    "# df.category = df.category.map(lambda x: \"WORLDPOST\" if x == \"THE WORLDPOST\" else x)\n",
    "# df['text'] = df['headline'] + '. ' + df['short_description']\n",
    "# df = df[['text', 'category']]\n",
    "\n",
    "categories = np.array(labels)\n",
    "text = np.array(texts)\n",
    "\n",
    "\n",
    "import re\n",
    "\n",
    "\n",
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for dataset\n",
    "    Every dataset is lower cased except\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"\\\\\", \"\", string)\n",
    "    string = re.sub(r\"\\'\", \"\", string)\n",
    "    string = re.sub(r\"\\\"\", \"\", string)\n",
    "    return string.strip().lower()\n",
    "\n",
    "paras = []\n",
    "labels = []\n",
    "texts = []\n",
    "\n",
    "sent_lens = []\n",
    "sent_nums = []\n",
    "for idx in range(text.shape[0]):\n",
    "    text = clean_str(df.text[idx])\n",
    "    texts.append(text)\n",
    "    sentences = tokenize.sent_tokenize(text)\n",
    "    sent_nums.append(len(sentences))\n",
    "    for sent in sentences:\n",
    "        sent_lens.append(len(text_to_word_sequence(sent)))\n",
    "    paras.append(sentences)\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_features, oov_token=True)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "data = np.zeros((len(texts), max_senten_num, max_senten_len), dtype='int32')\n",
    "for i, sentences in enumerate(paras):\n",
    "    for j, sent in enumerate(sentences):\n",
    "        if j < max_senten_num:\n",
    "            wordTokens = text_to_word_sequence(sent)\n",
    "            k = 0\n",
    "            for _, word in enumerate(wordTokens):\n",
    "                try:\n",
    "                    if k < max_senten_len and tokenizer.word_index[word] < max_features:\n",
    "                        data[i, j, k] = tokenizer.word_index[word]\n",
    "                        k = k + 1\n",
    "                except:\n",
    "                    print(word)\n",
    "                    pass\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Total %s unique tokens.' % len(word_index))\n",
    "\n",
    "labels = pd.get_dummies(categories)\n",
    "\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of labels tensor:', labels.shape)\n",
    "\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels.iloc[indices]\n",
    "nb_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "\n",
    "x_train = data[:-nb_validation_samples]\n",
    "y_train = labels[:-nb_validation_samples]\n",
    "x_val = data[-nb_validation_samples:]\n",
    "y_val = labels[-nb_validation_samples:]\n",
    "print('Number of positive and negative reviews in traing and validation set')\n",
    "print(y_train.columns.tolist())\n",
    "print(y_train.sum(axis=0).tolist())\n",
    "print(y_val.sum(axis=0).tolist())\n",
    "\n",
    "REG_PARAM = 1e-13\n",
    "l2_reg = regularizers.l2(REG_PARAM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(88000, 6, 40)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 400000 word vectors.\n",
      "Total absent words are 16061 which is 22.93 % of total words\n"
     ]
    }
   ],
   "source": [
    "GLOVE_DIR = \"data/glove.6B.100d.txt\"\n",
    "embeddings_index = {}\n",
    "f = open(GLOVE_DIR, encoding='UTF-8')\n",
    "for line in f:\n",
    "    try:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    except:\n",
    "        print(word)\n",
    "        pass\n",
    "f.close()\n",
    "print('Total %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, embed_size))\n",
    "absent_words = 0\n",
    "absent_set = set()\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        for j in str(word):\n",
    "            absent_set.add(j)\n",
    "        absent_words += 1\n",
    "print('Total absent words are', absent_words, 'which is', \"%0.2f\" % (absent_words * 100 / len(word_index)),\n",
    "      '% of total words')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0',\n",
       " '1',\n",
       " '2',\n",
       " '3',\n",
       " '4',\n",
       " '5',\n",
       " '6',\n",
       " '7',\n",
       " '8',\n",
       " '9',\n",
       " 'T',\n",
       " 'a',\n",
       " 'b',\n",
       " 'c',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'g',\n",
       " 'h',\n",
       " 'i',\n",
       " 'j',\n",
       " 'k',\n",
       " 'l',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'p',\n",
       " 'q',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'v',\n",
       " 'w',\n",
       " 'x',\n",
       " 'y',\n",
       " 'z',\n",
       " '\\x7f',\n",
       " '\\x80',\n",
       " '\\x94',\n",
       " '\\xa0',\n",
       " 'Â¡',\n",
       " 'Â¢',\n",
       " 'Â£',\n",
       " 'Â©',\n",
       " '\\xad',\n",
       " 'Â®',\n",
       " 'Â¯',\n",
       " 'Â°',\n",
       " 'Â±',\n",
       " 'Â´',\n",
       " 'Â·',\n",
       " 'Â¹',\n",
       " 'Â½',\n",
       " 'Â¿',\n",
       " 'Ã ',\n",
       " 'Ã¡',\n",
       " 'Ã¢',\n",
       " 'Ã£',\n",
       " 'Ã¤',\n",
       " 'Ã¥',\n",
       " 'Ã¦',\n",
       " 'Ã§',\n",
       " 'Ã¨',\n",
       " 'Ã©',\n",
       " 'Ãª',\n",
       " 'Ã«',\n",
       " 'Ã­',\n",
       " 'Ã®',\n",
       " 'Ã±',\n",
       " 'Ã³',\n",
       " 'Ã¶',\n",
       " 'Ã¸',\n",
       " 'Ãº',\n",
       " 'Ã»',\n",
       " 'Ã¼',\n",
       " 'Ä‡',\n",
       " 'Ä',\n",
       " 'Ä“',\n",
       " 'ÄŸ',\n",
       " 'Å',\n",
       " 'ÅŸ',\n",
       " 'Å«',\n",
       " 'É™',\n",
       " 'É›',\n",
       " 'Éª',\n",
       " 'Ëˆ',\n",
       " 'ËŒ',\n",
       " 'Ë',\n",
       " 'Ì€',\n",
       " 'Ì',\n",
       " 'Î¬',\n",
       " 'Î­',\n",
       " 'Î¯',\n",
       " 'Î±',\n",
       " 'Î³',\n",
       " 'Î´',\n",
       " 'Îµ',\n",
       " 'Î·',\n",
       " 'Î¹',\n",
       " 'Îº',\n",
       " 'Î»',\n",
       " 'Î¼',\n",
       " 'Î½',\n",
       " 'Î¾',\n",
       " 'Î¿',\n",
       " 'Ï',\n",
       " 'Ï‚',\n",
       " 'Ïƒ',\n",
       " 'Ï„',\n",
       " 'Ï…',\n",
       " 'Ï‰',\n",
       " 'ÏŒ',\n",
       " 'Ï',\n",
       " 'Ï',\n",
       " 'Ğ°',\n",
       " 'Ğ±',\n",
       " 'Ğ³',\n",
       " 'Ğµ',\n",
       " 'Ğº',\n",
       " 'Ğ»',\n",
       " 'Ğ¼',\n",
       " 'Ğ¾',\n",
       " 'Ñ',\n",
       " 'Ñ‚',\n",
       " 'Ñƒ',\n",
       " 'ÑŒ',\n",
       " 'Ñ',\n",
       " 'Ù',\n",
       " 'à¦†',\n",
       " 'à¦‡',\n",
       " 'à¦—',\n",
       " 'à¦Ÿ',\n",
       " 'à¦­',\n",
       " 'à¦°',\n",
       " 'à¦²',\n",
       " 'à¦¸',\n",
       " 'à¦¾',\n",
       " 'à¦¿',\n",
       " 'à§‹',\n",
       " 'à§',\n",
       " 'à² ',\n",
       " 'á´¥',\n",
       " 'áµ’',\n",
       " 'á¶…',\n",
       " 'á¶˜',\n",
       " '\\u2009',\n",
       " '\\u200a',\n",
       " '\\u200b',\n",
       " '\\u200d',\n",
       " '\\u200e',\n",
       " '\\u200f',\n",
       " 'â€',\n",
       " 'â€“',\n",
       " 'â€”',\n",
       " 'â€•',\n",
       " 'â€˜',\n",
       " 'â€™',\n",
       " 'â€œ',\n",
       " 'â€',\n",
       " 'â€¢',\n",
       " 'â€¦',\n",
       " '\\u2028',\n",
       " '\\u202a',\n",
       " '\\u202c',\n",
       " '\\u202f',\n",
       " 'â€²',\n",
       " 'â€³',\n",
       " 'â€¹',\n",
       " 'â€º',\n",
       " 'â‚¬',\n",
       " 'â„¢',\n",
       " 'â‰ ',\n",
       " 'âŠ™',\n",
       " 'â‘¡',\n",
       " 'â”€',\n",
       " 'â—•',\n",
       " 'â˜€',\n",
       " 'â˜ƒ',\n",
       " 'â˜”',\n",
       " 'â˜•',\n",
       " 'â˜˜',\n",
       " 'â™‚',\n",
       " 'â™¥',\n",
       " 'â™«',\n",
       " 'â™¬',\n",
       " 'âš¡',\n",
       " 'âš¾',\n",
       " 'â›±',\n",
       " 'â›½',\n",
       " 'âœ‚',\n",
       " 'âœˆ',\n",
       " 'âœŠ',\n",
       " 'âœŒ',\n",
       " 'âœ”',\n",
       " 'âœ¨',\n",
       " 'â„',\n",
       " 'â¤',\n",
       " 'â¡',\n",
       " 'ãƒ„',\n",
       " 'è±¡',\n",
       " 'ï¸',\n",
       " 'ï¸¿',\n",
       " '\\ufeff',\n",
       " 'ğŸ‡¦',\n",
       " 'ğŸ‡¨',\n",
       " 'ğŸ‡«',\n",
       " 'ğŸ‡¬',\n",
       " 'ğŸ‡­',\n",
       " 'ğŸ‡µ',\n",
       " 'ğŸ‡·',\n",
       " 'ğŸ‡¸',\n",
       " 'ğŸ‡º',\n",
       " 'ğŸ‡¿',\n",
       " 'ğŸŒˆ',\n",
       " 'ğŸŒŠ',\n",
       " 'ğŸŒ',\n",
       " 'ğŸŒ',\n",
       " 'ğŸŒ‘',\n",
       " 'ğŸŒ’',\n",
       " 'ğŸŒ“',\n",
       " 'ğŸŒ”',\n",
       " 'ğŸŒ•',\n",
       " 'ğŸŒ–',\n",
       " 'ğŸŒ—',\n",
       " 'ğŸŒ˜',\n",
       " 'ğŸŒ',\n",
       " 'ğŸŒ',\n",
       " 'ğŸŒ¤',\n",
       " 'ğŸŒ¦',\n",
       " 'ğŸŒ®',\n",
       " 'ğŸŒ³',\n",
       " 'ğŸŒ´',\n",
       " 'ğŸŒ·',\n",
       " 'ğŸŒ¸',\n",
       " 'ğŸŒ¹',\n",
       " 'ğŸŒ¿',\n",
       " 'ğŸ',\n",
       " 'ğŸ‚',\n",
       " 'ğŸƒ',\n",
       " 'ğŸ…',\n",
       " 'ğŸ†',\n",
       " 'ğŸ‹',\n",
       " 'ğŸŒ',\n",
       " 'ğŸ',\n",
       " 'ğŸ',\n",
       " 'ğŸ‘',\n",
       " 'ğŸ’',\n",
       " 'ğŸ“',\n",
       " 'ğŸ”',\n",
       " 'ğŸ•',\n",
       " 'ğŸ–',\n",
       " 'ğŸ—',\n",
       " 'ğŸœ',\n",
       " 'ğŸ',\n",
       " 'ğŸ',\n",
       " 'ğŸ©',\n",
       " 'ğŸª',\n",
       " 'ğŸ­',\n",
       " 'ğŸ°',\n",
       " 'ğŸ´',\n",
       " 'ğŸ·',\n",
       " 'ğŸ¸',\n",
       " 'ğŸ¹',\n",
       " 'ğŸ»',\n",
       " 'ğŸ½',\n",
       " 'ğŸ¾',\n",
       " 'ğŸ€',\n",
       " 'ğŸ',\n",
       " 'ğŸ‚',\n",
       " 'ğŸƒ',\n",
       " 'ğŸ„',\n",
       " 'ğŸ…',\n",
       " 'ğŸ†',\n",
       " 'ğŸˆ',\n",
       " 'ğŸ‰',\n",
       " 'ğŸŠ',\n",
       " 'ğŸ“',\n",
       " 'ğŸ¤',\n",
       " 'ğŸ®',\n",
       " 'ğŸµ',\n",
       " 'ğŸ¶',\n",
       " 'ğŸ¾',\n",
       " 'ğŸ€',\n",
       " 'ğŸƒ',\n",
       " 'ğŸ…',\n",
       " 'ğŸ†',\n",
       " 'ğŸˆ',\n",
       " 'ğŸŠ',\n",
       " 'ğŸ¨',\n",
       " 'ğŸ«',\n",
       " 'ğŸ°',\n",
       " 'ğŸ¼',\n",
       " 'ğŸ½',\n",
       " 'ğŸ¾',\n",
       " 'ğŸŠ',\n",
       " 'ğŸ‹',\n",
       " 'ğŸ',\n",
       " 'ğŸ',\n",
       " 'ğŸ”',\n",
       " 'ğŸ˜',\n",
       " 'ğŸ›',\n",
       " 'ğŸ',\n",
       " 'ğŸ­',\n",
       " 'ğŸ®',\n",
       " 'ğŸ±',\n",
       " 'ğŸ²',\n",
       " 'ğŸ¶',\n",
       " 'ğŸ·',\n",
       " 'ğŸ¸',\n",
       " 'ğŸ¾',\n",
       " 'ğŸ‘€',\n",
       " 'ğŸ‘…',\n",
       " 'ğŸ‘Š',\n",
       " 'ğŸ‘‹',\n",
       " 'ğŸ‘Œ',\n",
       " 'ğŸ‘',\n",
       " 'ğŸ‘',\n",
       " 'ğŸ‘‘',\n",
       " 'ğŸ‘“',\n",
       " 'ğŸ‘–',\n",
       " 'ğŸ‘ ',\n",
       " 'ğŸ‘©',\n",
       " 'ğŸ‘ª',\n",
       " 'ğŸ‘¬',\n",
       " 'ğŸ‘­',\n",
       " 'ğŸ‘¯',\n",
       " 'ğŸ‘°',\n",
       " 'ğŸ‘´',\n",
       " 'ğŸ‘µ',\n",
       " 'ğŸ‘¶',\n",
       " 'ğŸ‘¹',\n",
       " 'ğŸ‘»',\n",
       " 'ğŸ‘¼',\n",
       " 'ğŸ‘½',\n",
       " 'ğŸ’€',\n",
       " 'ğŸ’',\n",
       " 'ğŸ’ƒ',\n",
       " 'ğŸ’„',\n",
       " 'ğŸ’…',\n",
       " 'ğŸ’‡',\n",
       " 'ğŸ’‰',\n",
       " 'ğŸ’Š',\n",
       " 'ğŸ’‹',\n",
       " 'ğŸ’',\n",
       " 'ğŸ’',\n",
       " 'ğŸ’',\n",
       " 'ğŸ’”',\n",
       " 'ğŸ’•',\n",
       " 'ğŸ’–',\n",
       " 'ğŸ’—',\n",
       " 'ğŸ’˜',\n",
       " 'ğŸ’™',\n",
       " 'ğŸ’š',\n",
       " 'ğŸ’›',\n",
       " 'ğŸ’œ',\n",
       " 'ğŸ’',\n",
       " 'ğŸ’¡',\n",
       " 'ğŸ’¤',\n",
       " 'ğŸ’¥',\n",
       " 'ğŸ’¦',\n",
       " 'ğŸ’¨',\n",
       " 'ğŸ’©',\n",
       " 'ğŸ’ª',\n",
       " 'ğŸ’¯',\n",
       " 'ğŸ’°',\n",
       " 'ğŸ’¸',\n",
       " 'ğŸ’»',\n",
       " 'ğŸ’¼',\n",
       " 'ğŸ“š',\n",
       " 'ğŸ“',\n",
       " 'ğŸ“±',\n",
       " 'ğŸ“·',\n",
       " 'ğŸ“¸',\n",
       " 'ğŸ“º',\n",
       " 'ğŸ”‹',\n",
       " 'ğŸ”',\n",
       " 'ğŸ”‘',\n",
       " 'ğŸ”¥',\n",
       " 'ğŸ”ª',\n",
       " 'ğŸ”®',\n",
       " 'ğŸ•µ',\n",
       " 'ğŸ•º',\n",
       " 'ğŸ—£',\n",
       " 'ğŸ˜€',\n",
       " 'ğŸ˜',\n",
       " 'ğŸ˜‚',\n",
       " 'ğŸ˜ƒ',\n",
       " 'ğŸ˜‡',\n",
       " 'ğŸ˜ˆ',\n",
       " 'ğŸ˜‰',\n",
       " 'ğŸ˜Š',\n",
       " 'ğŸ˜',\n",
       " 'ğŸ˜',\n",
       " 'ğŸ˜',\n",
       " 'ğŸ˜‘',\n",
       " 'ğŸ˜’',\n",
       " 'ğŸ˜”',\n",
       " 'ğŸ˜•',\n",
       " 'ğŸ˜˜',\n",
       " 'ğŸ˜œ',\n",
       " 'ğŸ˜',\n",
       " 'ğŸ˜ ',\n",
       " 'ğŸ˜¡',\n",
       " 'ğŸ˜¢',\n",
       " 'ğŸ˜©',\n",
       " 'ğŸ˜¬',\n",
       " 'ğŸ˜­',\n",
       " 'ğŸ˜®',\n",
       " 'ğŸ˜°',\n",
       " 'ğŸ˜±',\n",
       " 'ğŸ˜³',\n",
       " 'ğŸ˜´',\n",
       " 'ğŸ˜µ',\n",
       " 'ğŸ˜¶',\n",
       " 'ğŸ˜·',\n",
       " 'ğŸ˜»',\n",
       " 'ğŸ™€',\n",
       " 'ğŸ™‚',\n",
       " 'ğŸ™ƒ',\n",
       " 'ğŸ™„',\n",
       " 'ğŸ™…',\n",
       " 'ğŸ™†',\n",
       " 'ğŸ™ˆ',\n",
       " 'ğŸ™Š',\n",
       " 'ğŸ™Œ',\n",
       " 'ğŸ™',\n",
       " 'ğŸš€',\n",
       " 'ğŸš‚',\n",
       " 'ğŸš‡',\n",
       " 'ğŸš—',\n",
       " 'ğŸš¨',\n",
       " 'ğŸš¬',\n",
       " 'ğŸš´',\n",
       " 'ğŸ›€',\n",
       " 'ğŸ¤‘',\n",
       " 'ğŸ¤“',\n",
       " 'ğŸ¤”',\n",
       " 'ğŸ¤—',\n",
       " 'ğŸ¤˜',\n",
       " 'ğŸ¤¢',\n",
       " 'ğŸ¤¦',\n",
       " 'ğŸ¤¼',\n",
       " 'ğŸ¥‚',\n",
       " 'ğŸ¥—',\n",
       " 'ğŸ¦„'}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "absent_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 88000 samples, validate on 22000 samples\n",
      "Epoch 1/50\n",
      "47104/88000 [===============>..............] - ETA: 30s - loss: 1.4430 - acc: 0.4343"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-44-83d218f0caee>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[0mcheckpoint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mModelCheckpoint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'best_model.h5'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmonitor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'val_loss'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_best_only\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'auto'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m512\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "embedding_layer = Embedding(len(word_index) + 1, embed_size, weights=[embedding_matrix], input_length=max_senten_len,\n",
    "                            trainable=False)\n",
    "\n",
    "word_input = Input(shape=(max_senten_len,), dtype='float32')\n",
    "word_sequences = embedding_layer(word_input)\n",
    "word_lstm = Bidirectional(CuDNNLSTM(150, return_sequences=True, kernel_regularizer=l2_reg))(word_sequences)\n",
    "word_dense = TimeDistributed(Dense(200, kernel_regularizer=l2_reg))(word_lstm)\n",
    "word_att = AttentionWithContext()(word_dense)\n",
    "wordEncoder = Model(word_input, word_att)\n",
    "\n",
    "sent_input = Input(shape=(max_senten_num, max_senten_len), dtype='float32')\n",
    "sent_encoder = TimeDistributed(wordEncoder)(sent_input)\n",
    "sent_lstm = Bidirectional(CuDNNLSTM(150, return_sequences=True, kernel_regularizer=l2_reg))(sent_encoder)\n",
    "sent_dense = TimeDistributed(Dense(200, kernel_regularizer=l2_reg))(sent_lstm)\n",
    "sent_att = Dropout(0.5)(AttentionWithContext()(sent_dense))\n",
    "preds = Dense(5, activation='softmax')(sent_att)\n",
    "model = Model(sent_input, preds)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "\n",
    "checkpoint = ModelCheckpoint('best_model.h5', verbose=0, monitor='val_loss', save_best_only=True, mode='auto')\n",
    "\n",
    "history = model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=50, batch_size=512, callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
