{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"model_generator.ipynb","version":"0.3.2","provenance":[{"file_id":"1gR6Ys1coMqGcIQqDlJRYZldQsT1vDFPc","timestamp":1552475641431},{"file_id":"1qD03bGnkXyFPx0L8daJqFPm9chvVQa7l","timestamp":1552468067741}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"OecanmfqD8Yv","colab_type":"code","colab":{}},"cell_type":"code","source":["\"\"\"\n","    References:  https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html\n","                 https://github.com/keras-team/keras/blob/master/examples\n","                 http://www.aaai.org/ocs/index.php/AAAI/AAAI15/paper/view/9745\n","                 https://github.com/airalcorn2/Recurrent-Convolutional-Neural-Network-Text-Classifier/blob/master/recurrent_convolutional_keras.py\n","\"\"\"\n","import os\n","import re\n","import nltk\n","import shutil\n","import math\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","from google.colab import files\n","import urllib.request\n","import numpy as np\n","import pandas as pd\n","import os\n","import string\n","import keras\n","from google.colab import drive\n","from sklearn.metrics import accuracy_score\n","from keras.models import load_model\n","from matplotlib import pyplot as plt\n","from nltk.corpus import stopwords\n","from keras.preprocessing.text import Tokenizer\n","from keras.utils import to_categorical\n","from keras.initializers import Constant\n","from keras.models import Model\n","from keras.callbacks import ModelCheckpoint\n","from keras import backend\n","from keras.optimizers import Adamax, Adam, Nadam\n","from keras.layers import BatchNormalization, AveragePooling1D, GlobalAveragePooling1D\n","from keras.layers import Conv1D, Dense, Input, Lambda, CuDNNLSTM, CuDNNGRU, BatchNormalization\n","from keras.layers import Bidirectional, GlobalMaxPooling1D, MaxPooling1D, Dropout, SpatialDropout1D\n","from keras.layers.merge import concatenate\n","from keras.layers.embeddings import Embedding\n","from keras.preprocessing.sequence import pad_sequences\n","from sklearn.model_selection import train_test_split"],"execution_count":0,"outputs":[]},{"metadata":{"id":"e-z5OPQmT0se","colab_type":"text"},"cell_type":"markdown","source":["### IMPORT FILES FROM GOOGLE DRIVE"]},{"metadata":{"id":"I5Z0VSArLhKc","colab_type":"code","colab":{}},"cell_type":"code","source":["file_paths = ['test.csv', 'valid.csv', 'train.csv', 'glove.6B.100d.txt']\n","# Connect to Google Drive\n","drive.mount('/content/gdrive')\n","base_url = 'gdrive/My Drive/'\n","\n","for path in file_paths:\n","    if not os.path.isfile(path):\n","        try:\n","            shutil.copy(base_url + \"data/\" + path, path)\n","        except:\n","            os.mkdir(base_url + \"data\")\n","            os.mkdir(base_url + \"data/valid_prediction\")\n","            os.mkdir(base_url + \"data/test_prediction\")\n","            print(path + \" Not Found in Google Drive\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"-qvIG5j0UQEK","colab_type":"text"},"cell_type":"markdown","source":["### DOWNLOAD FILES If NO FILES ARE FOUND IN GOOGLE DRIVE"]},{"metadata":{"id":"nVYk7Q4fSmj6","colab_type":"code","colab":{}},"cell_type":"code","source":["test_path = \"https://firebasestorage.googleapis.com/v0/b/hyungyu415.appspot.com/o/test.csv?alt=media&token=979d05cf-d252-4d15-8ec4-a8ed556c6fe9\"\n","valid_path = \"https://firebasestorage.googleapis.com/v0/b/hyungyu415.appspot.com/o/valid.csv?alt=media&token=deb7ef2d-1442-48b1-8e17-153aff0bed94\"\n","train_path = \"https://firebasestorage.googleapis.com/v0/b/hyungyu415.appspot.com/o/train.csv?alt=media&token=a2f73b7a-f471-4f4a-a15c-57714a6870b2\"\n","embedding_path = \"https://firebasestorage.googleapis.com/v0/b/ryan-blog415.appspot.com/o/glove.6B.100d.txt?alt=media&token=e234f3ae-f47a-45ec-a303-159ea4111fbb\"\n","download_paths = [test_path, valid_path, train_path, embedding_path]\n","file_link_dict = dict(zip(file_paths, download_paths))\n","for path in file_link_dict.keys():\n","    if not os.path.isfile(path):\n","        filedata = urllib.request.urlopen(file_link_dict[path])\n","        datatowrite = filedata.read()\n","\n","        with open(path, 'wb') as f:  \n","            f.write(datatowrite)\n","    else:\n","        print(\"Already Has %s\" % path)\n","\n","def filter_text(string):\n","    string = str(string)\n","    string = re.sub('\\[[^]]*\\]', '', string)\n","    string = string.replace('\\n', ' ')\n","    return string\n","\n","file_names = ['valid.csv', 'test.csv', 'train.csv']\n","for file_name in file_names:\n","    df = pd.read_csv(file_name)\n","    df['text'] = df['text'].apply(filter_text)\n","    extra = df['cool'] + df['funny'] + df['useful']\n","    df['extra'] = extra\n","    df.drop([i for i in df.columns if i not in ['extra', 'stars', 'text']], axis=1)\n","    df.to_csv(file_name, index=False)\n","    \n","for path in file_paths:\n","    shutil.copy(path, base_url + \"data/\" + path)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Ty3aDwmaUEh_","colab_type":"text"},"cell_type":"markdown","source":["### READ FILES FOR EMBEDDING AND DATA"]},{"metadata":{"id":"Dk7QGr7uEE1z","colab_type":"code","colab":{}},"cell_type":"code","source":["MAX_SEQUENCE_LENGTH = 250\n","MAX_NUM_WORDS = 30000\n","EMBEDDING_DIM = 100\n","VALIDATION_SPLIT = 20000/120000\n","\n","# first, build index mapping words in the embeddings set\n","# to their embedding vector\n","\n","print('Indexing word vectors.')\n","\n","embeddings_index = {}\n","with open(os.path.join('glove.6B.100d.txt'), encoding='UTF-8') as f:\n","    for line in f:\n","        values = line.split()\n","        word = values[0]\n","        coefs = np.asarray(values[1:], dtype='float32')\n","        embeddings_index[word] = coefs\n","\n","print('Found %s word vectors.' % len(embeddings_index))\n","\n","# second, prepare text samples and their labels\n","print('Processing text dataset')\n","\n","texts = []  # list of text samples\n","labels = []  # list of labels\n","extras = []\n","\n","files = ['train.csv', 'valid.csv', 'test.csv']\n","\n","for file_name in files:\n","    file = pd.read_csv(file_name)\n","    for i in range(len(file['text'])):\n","        if math.isnan(file['stars'][i]) or math.isnan(file['extra'][i]):\n","            continue\n","        texts.append(str(file['text'][i]))\n","        labels.append(int(file['stars'][i]))\n","        extras.append(int(file['extra'][i]))\n","\n","VALIDATION_SPLIT = 20000/len(texts)\n","\n","print('Found %s texts.' % len(texts))\n","\n","# finally, vectorize the text samples into a 2D integer tensor\n","tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n","tokenizer.fit_on_texts(texts)\n","sequences = tokenizer.texts_to_sequences(texts)\n","\n","word_index = tokenizer.word_index\n","print('Found %s unique tokens.' % len(word_index))\n","\n","data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n","\n","_, y_val_raw = train_test_split(labels, test_size=VALIDATION_SPLIT, shuffle=False)\n","y_val_raw, _ = train_test_split(y_val_raw, test_size=0.5, shuffle=False)\n","\n","labels = to_categorical(np.asarray(labels))\n","print('Shape of data tensor:', data.shape)\n","print('Shape of label tensor:', labels.shape)\n","\n","# split the data into a training set and a validation set\n","indices = np.arange(data.shape[0])\n","data = data[indices]\n","labels = labels[indices]\n","num_validation_samples = int(VALIDATION_SPLIT / 2 * data.shape[0])\n","num_test_samples = num_validation_samples\n","\n","x_train, x_test, y_train, y_test = train_test_split(data, labels, test_size=VALIDATION_SPLIT, shuffle=False)\n","x_val, x_test, y_val, y_test = train_test_split(x_test, y_test, test_size=0.5, shuffle=False)\n","x_extra, x_val_extra = train_test_split(extras, test_size=VALIDATION_SPLIT, shuffle=False)\n","x_val_extra, x_test_extra = train_test_split(x_val_extra, test_size=0.5, shuffle=False)\n","\n","print('Preparing embedding matrix.')\n","# prepare embedding matrix\n","num_words = min(MAX_NUM_WORDS, len(word_index)) + 1\n","embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n","for word, i in word_index.items():\n","    if i > MAX_NUM_WORDS:\n","        continue\n","    embedding_vector = embeddings_index.get(word)\n","    if embedding_vector is not None:\n","        # words not found in embedding index will be all-zeros.\n","        embedding_matrix[i] = embedding_vector"],"execution_count":0,"outputs":[]},{"metadata":{"id":"x7nS10fkWGyp","colab_type":"text"},"cell_type":"markdown","source":["### ADD EXTRA FEATURE (COOL + USEFUL + FUNNY)"]},{"metadata":{"id":"6EjdzeFuFjls","colab_type":"code","colab":{}},"cell_type":"code","source":["x_val_w_extra = x_val\n","x_w_extra = x_train\n","x_test_w_extra = x_test\n","feature_weight = 10\n","if check is None:\n","    check = True\n","\n","x_val_w_extra = list(x_val_w_extra)\n","for j in range(feature_weight):\n","    for i, each in enumerate(x_val_w_extra):\n","        each = list(each)\n","        each.append(x_val_extra[i])\n","        x_val_w_extra[i] = np.array(each)\n","x_val_w_extra = np.array(x_val_w_extra)\n","\n","x_w_extra = list(x_w_extra)\n","for j in range(feature_weight):\n","    for i, each in enumerate(x_w_extra):\n","        each = list(each)\n","        each.append(x_extra[i])\n","        x_w_extra[i] = np.array(each)\n","x_w_extra = np.array(x_w_extra)\n","\n","x_test_w_extra = list(x_test_w_extra)\n","for j in range(feature_weight):\n","    for i, each in enumerate(x_test_w_extra):\n","        each = list(each)\n","        each.append(x_test_extra[i])\n","        x_test_w_extra[i] = np.array(each)\n","x_test_w_extra = np.array(x_test_w_extra)\n","\n","if check:\n","    MAX_SEQUENCE_LENGTH = MAX_SEQUENCE_LENGTH + feature_weight\n","    check = False\n","x_test_w_extra.shape"],"execution_count":0,"outputs":[]},{"metadata":{"id":"gWOJ8NTWWOnO","colab_type":"text"},"cell_type":"markdown","source":["### CREATE MODEL BASED ON VALIDATION ACCURACY FOR SPECIFIED EPOCH COUNT AND SAVE TO GOOGLE DRIVE\n","\n","#### EPOCH = 15 (Default)\n","#### 5 Iterations (Default) --> 5 model creation total"]},{"metadata":{"id":"yzPI2drXD6Dm","colab_type":"code","colab":{}},"cell_type":"code","source":["epoch = 15\n","for i in range(20, 25):\n","    i = \"0%d\" % i if i < 10 else str(i)\n","    print('Training %s' % i)\n","    \n","    embedding_layer = Embedding(num_words,\n","                                EMBEDDING_DIM,\n","                                embeddings_initializer=Constant(embedding_matrix),\n","                                input_length=MAX_SEQUENCE_LENGTH,\n","                                trainable=False)\n","\n","    sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n","\n","    embedded_sequences = embedding_layer(sequence_input)\n","\n","    x = Dropout(0.3)(embedded_sequences)\n","    x = BatchNormalization()(x)\n","    x = Conv1D(128, 5, padding='valid', activation='relu', strides=1)(x)\n","    x = MaxPooling1D(5)(x)\n","    x = Bidirectional(CuDNNLSTM(units=150, return_sequences=True))(x)\n","    x = Bidirectional(CuDNNLSTM(units=150))(x)\n","    x = Dropout(0.3)(x)\n","\n","    preds = Dense(units=6, input_dim=150, activation='softmax')(x)\n","\n","    model_name = \"model%s.h5\" % i\n","#     checkpoint = ModelCheckpoint(model_name, monitor='val_acc', verbose=0,\n","#                                  save_best_only=True, mode='max')\n","    model = Model(inputs=sequence_input, outputs=preds)\n","    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n","    model.fit(x_w_extra, y_train, initial_epoch=0, # callbacks=[checkpoint],\n","              batch_size=250, epochs=epoch, verbose=2, validation_split=0.1)\n","    model.save(model_name)\n","    shutil.copy(model_name, base_url + \"model_2/\" + model_name)\n","#     print(model.evaluate(x_val_w_extra, y_val))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"u_0MUghqWeCx","colab_type":"text"},"cell_type":"markdown","source":["### EVALUATE ON MODELS GENERATED"]},{"metadata":{"id":"z2SeMFaPBv4z","colab_type":"code","colab":{}},"cell_type":"code","source":["# def find_max_index(some_list):\n","#     maximum = -100\n","#     index = -100\n","#     for i, v in enumerate(some_list):\n","#         if maximum < v:\n","#             index = i\n","#             maximum = v\n","#     return index\n","\n","# models = [i for i in os.listdir() if i.endswith(\".h5\")]\n","# models.sort()\n","# print(models)\n","# top_list = list()\n","# test_preds = None\n","# preds = None\n","# for model_name in models:\n","#     print(\"Loading %s\" % model_name)\n","#     model = load_model(model_name)\n","#     pred = model.predict(x_val_w_extra)\n","#     test_pred = model.predict(x_test_w_extra)\n","#     if preds is None:\n","#         preds = pred\n","#     else:\n","#         preds = preds + pred\n","#     if test_preds is None:\n","#         test_preds = test_pred\n","#     else:\n","#         test_preds = test_preds + test_pred\n","    \n","# for sub_list in preds:\n","#     index = find_max_index(sub_list)\n","#     top_list.append(index)\n","# acc_score = accuracy_score(y_val_raw, top_list)\n","# print(\"Accuracy: %.4f\" % acc_score)\n","\n","# top_list = list()\n","# for sub_list in test_preds:\n","#     index = find_max_index(sub_list)\n","#     top_list.append(index)\n","# print(top_list)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"5sIiHljXPbhm","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}